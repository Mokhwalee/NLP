{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù Chapter 1: Text Preprocessing\n",
    "\n",
    "## üìå Overview  \n",
    "Text preprocessing is the first and one of the most critical steps in any NLP pipeline. Raw text data is often messy and inconsistent, so cleaning and structuring the data makes it ready for machine learning models or statistical analysis.\n",
    "\n",
    "This chapter covers:\n",
    "- Text normalization\n",
    "- Tokenization\n",
    "- Stopword removal\n",
    "- Stemming and lemmatization\n",
    "- POS tagging\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Text Normalization  \n",
    "**Goal:** Standardize the text format to reduce variability.\n",
    "\n",
    "Common steps:\n",
    "- Lowercasing all text  \n",
    "- Removing punctuation and special characters  \n",
    "- Removing numbers (optional, task-dependent)  \n",
    "- Removing extra whitespace  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox  jumps over     lazy dogs \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Original sentence\n",
    "text = \"The Quick Brown Fox! Jumps over 123 lazy dogs.\"\n",
    "\n",
    "# Convert all characters to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove all characters that are NOT lowercase letters (a-z) or whitespace\n",
    "text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "\n",
    "# Print the cleaned text\n",
    "print(text)  # Output: \"the quick brown fox jumps over  lazy dogs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ Tokenization\n",
    "\n",
    "Goal: Split the text into individual units (tokens), typically words or subwords.\n",
    "\n",
    "Example using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/moka/Documents/GitHub/NLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/moka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/moka/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "import nltk  # Natural Language Toolkit\n",
    "nltk.download('punkt') # Download the punkt tokenizer model\n",
    "nltk.download('punkt_tab') # because it keeps giving error \"Resource punkt_tab not found\n",
    "\n",
    "from nltk.tokenize import word_tokenize  # Import word tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'fun', '!']\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "sentence = \"Natural Language Processing (NLP) is fun!\"\n",
    "\n",
    "# Apply tokenization\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Print the list of tokens\n",
    "print(tokens)  # Output: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'fun', '!']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ Stopword Removal\n",
    "\n",
    "Goal: Remove common words that usually don‚Äôt carry meaningful information like \"the,\" \"is,\" \"and.\"\n",
    "\n",
    "Example using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fun', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/moka/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords  # Import stopword list\n",
    "nltk.download('stopwords')  # Download the English stopwords\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter out stopwords from the tokenized words\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Print the filtered tokens (stopwords removed)\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4Ô∏è‚É£ Stemming and Lemmatization\n",
    "\n",
    "Both techniques reduce words to their root form:\n",
    "\n",
    "- Stemming: Applies heuristic rules (may not produce actual words).\n",
    "- Lemmatization: Uses vocabulary and grammar rules (returns valid words).\n",
    "\n",
    "Example using NLTK Stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "fli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer  # Import the Porter Stemmer\n",
    "\n",
    "# Create a stemmer instance\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming\n",
    "print(stemmer.stem('running'))  # Output: 'run'\n",
    "print(stemmer.stem('flies'))    # Output: 'fli' (note: not always a valid word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using NLTK Lemmatizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/moka/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "fly\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer  # Import the lemmatizer\n",
    "nltk.download('wordnet')  # Download WordNet data for lemmatization\n",
    "\n",
    "# Create a lemmatizer instance\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization (specify part-of-speech for accuracy)\n",
    "print(lemmatizer.lemmatize('running', pos='v'))  # Output: 'run'\n",
    "print(lemmatizer.lemmatize('flies', pos='n'))    # Output: 'fly'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5Ô∏è‚É£ Part-of-Speech (POS) Tagging\n",
    "\n",
    "Goal: Assign a grammatical category (noun, verb, adjective, etc.) to each token.\n",
    "\n",
    "Example using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']\n",
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/moka/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/moka/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')  # Download POS tagger model\n",
    "nltk.download('averaged_perceptron_tagger_eng') # Download English POS tagger model\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "\n",
    "# Apply POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Print the tokens with their corresponding POS tags\n",
    "print(pos_tags)\n",
    "# Example output: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practice Questions and Answers\n",
    "\n",
    "### 1. Why is tokenization an important step in NLP?\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units, such as words, subwords, or sentences.  \n",
    "It is important because:\n",
    "- It transforms raw text into structured data that models can process.\n",
    "- Many NLP tasks, like text classification or sentiment analysis, operate on individual words or tokens.\n",
    "- It helps handle punctuation, contractions, and special characters appropriately.\n",
    "- Without tokenization, the model would not be able to understand where one word ends and another begins.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What is the difference between stemming and lemmatization?\n",
    "\n",
    "|                | Stemming                              | Lemmatization                               |\n",
    "|----------------|----------------------------------------|----------------------------------------------|\n",
    "| **Definition** | Cuts off word endings to reduce words to their base/root form (often not a real word). | Reduces words to their dictionary form (lemma) using linguistic rules. |\n",
    "| **Example**    | \"running\" ‚Üí \"run\", \"flies\" ‚Üí \"fli\"     | \"running\" ‚Üí \"run\", \"flies\" ‚Üí \"fly\"           |\n",
    "| **Approach**   | Rule-based, fast, and heuristic        | More accurate, uses vocabulary and grammar   |\n",
    "| **Result**     | May produce non-meaningful roots       | Produces meaningful base forms              |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Can stopword removal hurt model performance in some tasks? Why or why not?\n",
    "\n",
    "Yes, stopword removal can hurt model performance depending on the task.\n",
    "\n",
    "- In tasks like **sentiment analysis**, words like *\"not\"* or *\"never\"* are important for understanding meaning and sentiment. Removing them could reverse or obscure the sentiment.\n",
    "- In **language modeling** or **machine translation**, stopwords contribute to sentence structure and meaning, so removing them might reduce accuracy.\n",
    "- However, for tasks like **topic modeling** or **information retrieval**, stopword removal often helps by reducing noise.\n",
    "\n",
    "**Conclusion:** Whether stopword removal is beneficial depends on the specific NLP task and the importance of those words for meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
