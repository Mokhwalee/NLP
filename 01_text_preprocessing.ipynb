{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù Chapter 1: Text Preprocessing\n",
    "\n",
    "## üìå Overview  \n",
    "Text preprocessing is the first and one of the most critical steps in any NLP pipeline. Raw text data is often messy and inconsistent, so cleaning and structuring the data makes it ready for machine learning models or statistical analysis.\n",
    "\n",
    "This chapter covers:\n",
    "- Text normalization\n",
    "- Tokenization\n",
    "- Stopword removal\n",
    "- Stemming and lemmatization\n",
    "- POS tagging\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Text Normalization  \n",
    "**Goal:** Standardize the text format to reduce variability.\n",
    "\n",
    "Common steps:\n",
    "- Lowercasing all text  \n",
    "- Removing punctuation and special characters  \n",
    "- Removing numbers (optional, task-dependent)  \n",
    "- Removing extra whitespace  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox jumps over  lazy dogs\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Original sentence\n",
    "text = \"The Quick Brown Fox! Jumps over 123 lazy dogs.\"\n",
    "\n",
    "# Convert all characters to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove all characters that are NOT lowercase letters (a-z) or whitespace\n",
    "text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "# Print the cleaned text\n",
    "print(text)  # Output: \"the quick brown fox jumps over  lazy dogs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ Tokenization\n",
    "\n",
    "Goal: Split the text into individual units (tokens), typically words or subwords.\n",
    "\n",
    "Example using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/moka/Documents/GitHub/NLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/moka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/moka/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "import nltk  # Natural Language Toolkit\n",
    "nltk.download('punkt') # Download the punkt tokenizer model\n",
    "nltk.download('punkt_tab') # because it keeps giving error \"Resource punkt_tab not found\n",
    "\n",
    "from nltk.tokenize import word_tokenize  # Import word tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'fun', '!']\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "sentence = \"Natural Language Processing (NLP) is fun!\"\n",
    "\n",
    "# Apply tokenization\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Print the list of tokens\n",
    "print(tokens)  # Output: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'fun', '!']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ Stopword Removal\n",
    "\n",
    "Goal: Remove common words that usually don‚Äôt carry meaningful information like \"the,\" \"is,\" \"and.\"\n",
    "\n",
    "Example using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'fun', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/moka/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords  # Import stopword list\n",
    "nltk.download('stopwords')  # Download the English stopwords\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter out stopwords from the tokenized words\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Print the filtered tokens (stopwords removed)\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4Ô∏è‚É£ Stemming and Lemmatization\n",
    "\n",
    "Both techniques reduce words to their root form:\n",
    "\n",
    "Stemming: Applies heuristic rules (may not produce actual words).\n",
    "Lemmatization: Uses vocabulary and grammar rules (returns valid words).\n",
    "Example using NLTK Stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "fli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer  # Import the Porter Stemmer\n",
    "\n",
    "# Create a stemmer instance\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming\n",
    "print(stemmer.stem('running'))  # Output: 'run'\n",
    "print(stemmer.stem('flies'))    # Output: 'fli' (note: not always a valid word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example using NLTK Lemmatizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/moka/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "fly\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer  # Import the lemmatizer\n",
    "nltk.download('wordnet')  # Download WordNet data for lemmatization\n",
    "\n",
    "# Create a lemmatizer instance\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization (specify part-of-speech for accuracy)\n",
    "print(lemmatizer.lemmatize('running', pos='v'))  # Output: 'run'\n",
    "print(lemmatizer.lemmatize('flies', pos='n'))    # Output: 'fly'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5Ô∏è‚É£ Part-of-Speech (POS) Tagging\n",
    "\n",
    "Goal: Assign a grammatical category (noun, verb, adjective, etc.) to each token.\n",
    "\n",
    "Example using NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/moka/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/moka/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')  # Download POS tagger model\n",
    "nltk.download('averaged_perceptron_tagger_eng') # Download English POS tagger model\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Apply POS tagging\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Print the tokens with their corresponding POS tags\n",
    "print(pos_tags)\n",
    "# Example output: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practice Questions:\n",
    "1. Why is tokenization an important step in NLP?\n",
    "2. What is the difference between stemming and lemmatization?\n",
    "3. Can stopword removal hurt model performance in some tasks? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
