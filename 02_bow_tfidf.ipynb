{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¶ Chapter 2: Text Vectorization (Bag of Words (BoW) and TF-IDF)\n",
    "\n",
    "## üìå Overview  \n",
    "Machine learning models require numerical inputs, but raw text is unstructured. To bridge this gap, we use text vectorization techniques like:\n",
    "- **Bag of Words (BoW)**: Counts the frequency of words in a document.\n",
    "- **TF-IDF (Term Frequency-Inverse Document Frequency)**: Adjusts raw counts based on how common or rare a word is across multiple documents.\n",
    "\n",
    "These methods help transform text into structured feature vectors for machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Bag of Words (BoW)  \n",
    "**Goal:** Represent each document as a vector of word counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and' 'fun' 'interesting' 'is' 'language' 'learning' 'machine' 'natural'\n",
      " 'processing']\n",
      "BoW Matrix:\n",
      " [[0 0 1 1 1 0 0 1 1]\n",
      " [0 1 0 1 0 1 1 0 0]\n",
      " [1 0 0 0 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  # Import BoW vectorizer\n",
    "\n",
    "# Example corpus (a list of text documents)\n",
    "corpus = [\n",
    "    \"Natural language processing is interesting\",\n",
    "    \"Machine learning is fun\",\n",
    "    \"Natural language processing and machine learning\"\n",
    "]\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the corpus and transform the text data into BoW matrix\n",
    "X_bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Show the vocabulary (feature names)\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Convert the sparse matrix to an array for readability\n",
    "print(\"BoW Matrix:\\n\", X_bow.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "Goal: Weigh words by their importance:\n",
    "\n",
    "Term Frequency (TF): How often a word appears in a document.\n",
    "\n",
    "Inverse Document Frequency (IDF): Downweights common words across all documents.\n",
    "\n",
    "Example using TfidfVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and' 'fun' 'interesting' 'is' 'language' 'learning' 'machine' 'natural'\n",
      " 'processing']\n",
      "TF-IDF Matrix:\n",
      " [[0.         0.         0.54935123 0.41779577 0.41779577 0.\n",
      "  0.         0.41779577 0.41779577]\n",
      " [0.         0.60465213 0.         0.45985353 0.         0.45985353\n",
      "  0.45985353 0.         0.        ]\n",
      " [0.50689001 0.         0.         0.         0.38550292 0.38550292\n",
      "  0.38550292 0.38550292 0.38550292]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Import TF-IDF vectorizer\n",
    "\n",
    "# Reusing the same corpus\n",
    "corpus = [\n",
    "    \"Natural language processing is interesting\",\n",
    "    \"Machine learning is fun\",\n",
    "    \"Natural language processing and machine learning\"\n",
    "]\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus into TF-IDF matrix\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Show the vocabulary (feature names)\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Convert the sparse matrix to an array for readability\n",
    "print(\"TF-IDF Matrix:\\n\", X_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÖ How to Interpret:\n",
    "In BoW, the numbers are just raw word counts.\n",
    "In TF-IDF, higher values mean that a word is important in that document but rare across others.\n",
    "\n",
    "## üß© Comparing BoW vs. TF-IDF\n",
    "\n",
    "| Aspect          | Bag of Words                     | TF-IDF                                    |\n",
    "|-----------------|----------------------------------|--------------------------------------------|\n",
    "| Counts          | Raw frequency of words          | Weighted by word importance (rarity)      |\n",
    "| Common Words    | Frequent words have large values| Common words are downweighted             |\n",
    "| Sparse          | Yes                              | Yes                                       |\n",
    "| Use Case        | Simple models, baseline methods | More meaningful for text classification   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus:\n",
    "1. \"Natural language processing is interesting\"\n",
    "2. \"Machine learning is fun\"\n",
    "3. \"Natural language processing and machine learning\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üßÆ Example Bag of Words (BoW) Matrix:\n",
    "| Document | and | fun | interesting | is | language | learning | machine | natural | processing |\n",
    "|----------|-----|-----|-------------|----|----------|----------|---------|---------|------------|\n",
    "| Doc 1    | 0   | 0   | 1           | 1  | 1        | 0        | 0       | 1       | 1          |\n",
    "| Doc 2    | 0   | 1   | 0           | 1  | 0        | 1        | 1       | 0       | 0          |\n",
    "| Doc 3    | 1   | 0   | 0           | 0  | 1        | 1        | 1       | 1       | 1          |\n",
    "\n",
    "üßÆ Example TF-IDF Matrix (simplified values for illustration):\n",
    "| Document | and   | fun   | interesting | is    | language | learning | machine | natural | processing |\n",
    "|----------|-------|-------|-------------|-------|----------|----------|---------|---------|------------|\n",
    "| Doc 1    | 0     | 0     | 0.62        | 0.31  | 0.31     | 0        | 0       | 0.31    | 0.62       |\n",
    "| Doc 2    | 0     | 0.58  | 0           | 0.29  | 0        | 0.58     | 0.58    | 0       | 0          |\n",
    "| Doc 3    | 0.40  | 0     | 0           | 0     | 0.20     | 0.40     | 0.40    | 0.20    | 0.20       |\n",
    "\n",
    "‚ö†Ô∏è Note: These TF-IDF values are illustrative and may differ slightly depending on the actual calculation formula (scikit-learn normalizes these vectors by L2 norm).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
