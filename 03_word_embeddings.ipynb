{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåê Chapter 3: Word Embeddings\n",
    "\n",
    "## üìå Overview  \n",
    "(Recall) Bag of Words (BoW) and TF-IDF represent words as sparse vectors where each dimension corresponds to a unique word. However, they fail to capture **semantic relationships** between words (e.g., \"king\" and \"queen\").\n",
    "\n",
    "**Word Embeddings** Word embeddings are a way to represent words as numerical vectors, where words with similar meanings or contexts are represented by vectors that are close together in space. This allows computers to understand and analyze the semantic relationships between words in a way that's more intuitive than simply using one-hot encoding or other basic methods. \n",
    "\n",
    " To represent the complexity of a typical 50,000 word English vocabulary requires hundreds of features. Designing all those features by hand, and assigning accurate coordinates to all those words, would be a lot of work! Therefore, A typical embedding might use a 300 dimensional space, so each word would be represented by 300 numbers.\n",
    "\n",
    "- Reference : https://www.cs.cmu.edu/~dst/WordEmbeddingDemo/tutorial.html\n",
    "\n",
    "**Word Embeddings** solve this problem by mapping words into dense, low-dimensional vectors where similar words have similar representations. These embeddings are learned from large text corpora.\n",
    "\n",
    "Popular embedding methods:\n",
    "- Word2Vec (Skip-gram, CBOW)\n",
    "    - **CBOW**: Predict center word from context words.\n",
    "    - **Skip-Gram**: Predict context words from center word.\n",
    "- GloVe (Global Vectors)\n",
    "- FastText\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Key Components\n",
    "| Component      | Meaning                                              |\n",
    "|----------------|------------------------------------------------------|\n",
    "| One-hot vector | Input where only one element is 1, others are 0       |\n",
    "| W1 (input)     | (V x d) matrix: one-hot ‚Üí hidden dense vector         |\n",
    "| W2 (output)    | (d x V) matrix: hidden vector ‚Üí output vocabulary     |\n",
    "\n",
    "## 3. Mathematical Flow\n",
    "Given:\n",
    "- Vocabulary size = **V**\n",
    "- Embedding size = **d**\n",
    "\n",
    "**Step-by-Step Computation:**\n",
    "1. Input: One-hot vector x (size V)\n",
    "2. Hidden Layer: h = W1·µÄ x\n",
    "3. Output Layer: u = W2·µÄ h\n",
    "4. Softmax: p = softmax(u)\n",
    "5. Loss: Cross-entropy\n",
    "6. Update W1 and W2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üöÄ Word2Vec (Skip-gram with Negative Sampling)\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Vocabulary and Indexing\n",
    "\n",
    "Given corpus: `\"The cat sits\"`  \n",
    "Vocabulary size $V = 3$.\n",
    "\n",
    "| Word   | Index |\n",
    "|--------|-------|\n",
    "| the    | 0     |\n",
    "| cat    | 1     |\n",
    "| sits   | 2     |\n",
    "\n",
    "---\n",
    "\n",
    "### üü° Step 1: Input Embedding Matrix $W$ (Center Words)\n",
    "\n",
    "- Shape: $V \\times d = 3 \\times 2$\n",
    "- Each **row** represents the embedding vector for a word **when it is the center word**.\n",
    "\n",
    "$$\n",
    "W =\n",
    "\\begin{bmatrix}\n",
    "0.2 & -0.1 \\\\\n",
    "0.7 & 0.3 \\\\\n",
    "-0.5 & 0.6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- $u_{\\text{the}} = [0.2, -0.1]$\n",
    "- $u_{\\text{cat}} = [0.7, 0.3]$\n",
    "- $u_{\\text{sits}} = [-0.5, 0.6]$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Mathematical Concept:**\n",
    "The input embedding matrix $W$ maps one-hot encoded center words to dense embeddings.  \n",
    "For a one-hot vector $x_{w_t}$ for center word $w_t$:\n",
    "\n",
    "$$\n",
    "u_{w_t} = W^\\top x_{w_t}\n",
    "$$\n",
    "\n",
    "This operation selects the $w_t$-th row of $W$ directly.\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Step 2: Output Embedding Matrix $W'$ (Context Words, different from the input weights W)\n",
    "\n",
    "- Shape: $V \\times d = 3 \\times 2$\n",
    "- Each **row** represents the embedding vector for a word **when it is a context word**.\n",
    "\n",
    "$$\n",
    "W' =\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.4 \\\\\n",
    "-0.3 & 0.5 \\\\\n",
    "0.6 & -0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- $v_{\\text{the}} = [0.1, 0.4]$\n",
    "- $v_{\\text{cat}} = [-0.3, 0.5]$\n",
    "- $v_{\\text{sits}} = [0.6, -0.2]$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Mathematical Concept:**\n",
    "The output embedding matrix $W'$ holds context word vectors.  \n",
    "During prediction, the center word's embedding (from $W$) is compared with these context vectors (from $W'$) via dot product.\n",
    "\n",
    "---\n",
    "\n",
    "### üü£ Step 3: Dot Product Calculation (Score Computation)\n",
    "\n",
    "Center word = `\"cat\"` (index 1):\n",
    "\n",
    "$$\n",
    "u_{\\text{cat}} = [0.7, 0.3]\n",
    "$$\n",
    "\n",
    "The score between center word $u_{\\text{cat}}$ and each context word $v_{w_o}$ is:\n",
    "\n",
    "$$\n",
    "s_{w_o} = u_{\\text{cat}}^\\top v_{w_o}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Detailed Calculations:**\n",
    "\n",
    "1. Between `\"cat\"` (center) and `\"the\"` (context):\n",
    "\n",
    "$$\n",
    "s_{\\text{the}} = (0.7 \\times 0.1) + (0.3 \\times 0.4) = 0.07 + 0.12 = 0.19\n",
    "$$\n",
    "\n",
    "2. Between `\"cat\"` (center) and `\"cat\"` (context):\n",
    "\n",
    "$$\n",
    "s_{\\text{cat}} = (0.7 \\times -0.3) + (0.3 \\times 0.5) = -0.21 + 0.15 = -0.06\n",
    "$$\n",
    "\n",
    "3. Between `\"cat\"` (center) and `\"sits\"` (context):\n",
    "\n",
    "$$\n",
    "s_{\\text{sits}} = (0.7 \\times 0.6) + (0.3 \\times -0.2) = 0.42 - 0.06 = 0.36\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Step 4: Resulting Raw Scores Table\n",
    "\n",
    "| Center Word | Context Word | Dot Product Score |\n",
    "|-------------|--------------|------------------|\n",
    "| cat         | the          | 0.19             |\n",
    "| cat         | cat          | -0.06            |\n",
    "| cat         | sits         | 0.36             |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° Mathematical Concept Behind Dot Product Scoring\n",
    "\n",
    "The dot product measures **alignment** or **similarity** between the embedding vectors of the center and context words:\n",
    "\n",
    "$$\n",
    "s_{w_o} = \\sum_{i=1}^{d} u_{w_t}^{(i)} \\cdot v_{w_o}^{(i)}\n",
    "$$\n",
    "\n",
    "- Large positive dot product ‚Üí vectors point in a similar direction (high similarity).\n",
    "- Zero ‚Üí vectors are orthogonal (unrelated).\n",
    "- Negative ‚Üí vectors point in opposite directions (dissimilar).\n",
    "\n",
    "These scores are:\n",
    "- Passed into the **softmax function** to obtain probabilities:\n",
    "\n",
    "$$\n",
    "p(w_o \\mid w_t) = \\frac{\\exp(s_{w_o})}{\\sum_{w=1}^{V} \\exp(s_w)}\n",
    "$$\n",
    "\n",
    "- Or used directly for **negative sampling loss** in more efficient training.\n",
    "\n",
    "#### Apply Softmax to Get Probabilities\n",
    "\n",
    "Softmax formula:\n",
    "\n",
    "$$\n",
    "p(w_o \\mid w_t) = \\frac{\\exp(s_{w_o})}{\\sum_{w=1}^{V} \\exp(s_{w})}\n",
    "$$\n",
    "\n",
    "Exponentials:\n",
    "- $\\exp(0.19) \\approx 1.209$\n",
    "- $\\exp(-0.06) \\approx 0.942$\n",
    "- $\\exp(0.36) \\approx 1.433$\n",
    "\n",
    "Denominator:\n",
    "\n",
    "$$\n",
    "1.209 + 0.942 + 1.433 = 3.584\n",
    "$$\n",
    "\n",
    "Probabilities:\n",
    "- $p(\\text{the} \\mid \\text{cat}) \\approx 0.337$\n",
    "- $p(\\text{cat} \\mid \\text{cat}) \\approx 0.263$\n",
    "- $p(\\text{sits} \\mid \\text{cat}) \\approx 0.400$\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Step 5: Calculate Loss (True Context is \"the\")\n",
    "\n",
    "Loss:\n",
    "\n",
    "$$\n",
    "L = -\\log p(\\text{the} \\mid \\text{cat})\n",
    "$$\n",
    "\n",
    "Substituting:\n",
    "\n",
    "$$\n",
    "L = -\\log(0.337) \\approx 1.087\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Step 6: Negative Sampling Alternative (Optional)\n",
    "\n",
    "If using negative sampling:\n",
    "- Positive pair: (\"cat\", \"the\")\n",
    "- Negative pairs: (\"cat\", \"cat\") and (\"cat\", \"sits\")\n",
    "\n",
    "Loss:\n",
    "\n",
    "$$\n",
    "L = -\\log \\sigma(u_{\\text{cat}}^\\top v_{\\text{the}}) - \\sum_{i=1}^{k} \\log \\sigma(-u_{\\text{cat}}^\\top v_{\\text{neg}, i})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Conclusion\n",
    "\n",
    "- The model maximizes dot product scores for correct pairs.\n",
    "- Reduces scores for incorrect pairs.\n",
    "- Learns meaningful word embeddings where similar words have similar vector representations.\n",
    "\n",
    "üß† Why This Works:\n",
    "\n",
    "- The model **encourages higher scores** for true (center, context) pairs.\n",
    "- It **reduces scores** for randomly sampled negative (incorrect) pairs.\n",
    "- Over time, embeddings for words appearing together in similar contexts **move closer in the vector space**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Full Step-by-Step with Full Mathematical Analysis + Examples\n",
    "## 1. Setup\n",
    "\n",
    "Vocabulary = `[\"apple\", \"banana\", \"cat\", \"dog\"]`\n",
    "\n",
    "| Word     | Index |\n",
    "|:---------|:------|\n",
    "| apple    | 0     |\n",
    "| banana   | 1     |\n",
    "| cat      | 2     |\n",
    "| dog      | 3     |\n",
    "\n",
    "Vocabulary size **V = 4**\n",
    "Embedding size **d = 2**\n",
    "\n",
    "Initial weight matrices:\n",
    "\n",
    "**W1** (V √ó d):\n",
    "\n",
    "| Word   | dim1  | dim2  |\n",
    "|--------|-------|-------|\n",
    "| apple  | 0.5   | -0.2  |\n",
    "| banana | 0.3   | 0.8   |\n",
    "| cat    | -0.4  | -0.4  |\n",
    "| dog    | 1.2   | 0.5   |\n",
    "\n",
    "**W2** (d √ó V):\n",
    "\n",
    "| dim    | apple | banana | cat  | dog  |\n",
    "|--------|-------|--------|------|------|\n",
    "| dim1   | -0.3  | 0.6    | -0.5 | -0.4 |\n",
    "| dim2   | 0.2   | -0.9   | -1.0 | -0.2 |\n",
    "## 2. Iteration 1: Center Word = `\"cat\"`, Context Word = `\"dog\"`\n",
    "\n",
    "### Step 1: Create One-Hot Vector\n",
    "\n",
    "```\n",
    "x = [0, 0, 1, 0]\n",
    "```\n",
    "\n",
    "### Step 2: Hidden Layer Calculation\n",
    "\n",
    "```\n",
    "h = W1·µÄ x = W1[\"cat\"] = [-0.4, -0.4]\n",
    "```\n",
    "\n",
    "### Step 3: Output Layer Calculation\n",
    "\n",
    "```\n",
    "u = W2·µÄ h\n",
    "```\n",
    "\n",
    "Softmax logits:\n",
    "\n",
    "```\n",
    "u = [0.04, 0.12, 0.6, 0.24]\n",
    "```\n",
    "\n",
    "### Step 4: Softmax Calculation\n",
    "\n",
    "```\n",
    "p = [0.198, 0.214, 0.346, 0.242]\n",
    "```\n",
    "\n",
    "### Step 5: Loss Calculation\n",
    "\n",
    "Cross-Entropy Loss:\n",
    "\n",
    "```\n",
    "Loss = -log(p_dog) ‚âà 1.418\n",
    "```\n",
    "\n",
    "### Step 6: Backpropagation\n",
    "\n",
    "Error Vector:\n",
    "\n",
    "```\n",
    "error = p - y = [0.198, 0.214, 0.346, -0.758]\n",
    "```\n",
    "\n",
    "Gradients:\n",
    "\n",
    "- `grad_W2 = h outer error`\n",
    "- `grad_W1 = W2 @ error`\n",
    "\n",
    "### Step 7: Update W1 and W2\n",
    "\n",
    "Gradient descent:\n",
    "\n",
    "```\n",
    "W1[\"cat\"] = W1[\"cat\"] - learning_rate * grad_W1\n",
    "W2 = W2 - learning_rate * grad_W2\n",
    "```\n",
    "## 3. Full Mathematical Explanation of Cross-Entropy and Softmax\n",
    "\n",
    "### 1. Softmax Function\n",
    "\n",
    "```\n",
    "softmax(u·µ¢) = exp(u·µ¢) / Œ£ exp(u‚±º)\n",
    "```\n",
    "\n",
    "### 2. Cross-Entropy Loss\n",
    "\n",
    "```\n",
    "L = -sum_i (y·µ¢ * log(p·µ¢))\n",
    "```\n",
    "\n",
    "Simplified when y is one-hot:\n",
    "\n",
    "```\n",
    "L = -log(p‚Çñ)\n",
    "```\n",
    "\n",
    "### 3. Derivative of Cross-Entropy + Softmax\n",
    "\n",
    "```\n",
    "‚àÇL/‚àÇu·µ¢ = p·µ¢ - y·µ¢\n",
    "```\n",
    "\n",
    "Thus:\n",
    "\n",
    "```\n",
    "error = p - y\n",
    "```\n",
    "\n",
    "### 4. Gradients\n",
    "\n",
    "- Gradient w.r.t W2:\n",
    "  \n",
    "```\n",
    "‚àÇL/‚àÇW2 = h outer (p - y)\n",
    "```\n",
    "\n",
    "- Gradient w.r.t W1:\n",
    "  \n",
    "```\n",
    "‚àÇL/‚àÇh = W2 (p - y)\n",
    "```\n",
    "\n",
    "thus only update W1[center_word].\n",
    "\n",
    "### 5. Update Rules\n",
    "\n",
    "```\n",
    "W2 = W2 - learning_rate * grad_W2\n",
    "W1[center_word] = W1[center_word] - learning_rate * grad_W1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden vector h: [-0.4 -0.4]\n",
      "Logits u: [0.04 0.12 0.6  0.24]\n",
      "Predicted probabilities p: [0.19780976 0.21428475 0.3463001  0.24160538]\n",
      "Loss: 1.420449526685425\n",
      "Error vector: [ 0.19780976  0.21428475  0.3463001  -0.75839462]\n",
      "Gradient W2:\n",
      " [[-0.0791239  -0.0857139  -0.13852004  0.30335785]\n",
      " [-0.0791239  -0.0857139  -0.13852004  0.30335785]]\n",
      "Gradient W1: [ 0.19943572 -0.34791551]\n",
      "\n",
      "Updated W1:\n",
      " [[ 0.5        -0.2       ]\n",
      " [ 0.3         0.8       ]\n",
      " [-0.41994357 -0.36520845]\n",
      " [ 1.2         0.5       ]]\n",
      "Updated W2:\n",
      " [[-0.29208761  0.60857139 -0.486148   -0.43033578]\n",
      " [ 0.20791239 -0.89142861 -0.986148   -0.23033578]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set initial W1 and W2\n",
    "W1 = np.array([\n",
    "    [0.5, -0.2],\n",
    "    [0.3, 0.8],\n",
    "    [-0.4, -0.4],\n",
    "    [1.2, 0.5]\n",
    "])\n",
    "\n",
    "W2 = np.array([\n",
    "    [-0.3, 0.6, -0.5, -0.4],\n",
    "    [0.2, -0.9, -1.0, -0.2]\n",
    "])\n",
    "\n",
    "vocab = [\"apple\", \"banana\", \"cat\", \"dog\"]\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "# Center word = \"cat\", Context word = \"dog\"\n",
    "center_word = \"cat\"\n",
    "context_word = \"dog\"\n",
    "\n",
    "# One-hot vector\n",
    "V = 4\n",
    "x = np.zeros(V)\n",
    "x[word_to_index[center_word]] = 1\n",
    "\n",
    "# Hidden layer\n",
    "h = W1.T @ x\n",
    "print(\"Hidden vector h:\", h)\n",
    "\n",
    "# Output logits\n",
    "u = W2.T @ h\n",
    "print(\"Logits u:\", u)\n",
    "\n",
    "# Softmax\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z))\n",
    "    return e_z / e_z.sum()\n",
    "\n",
    "p = softmax(u)\n",
    "print(\"Predicted probabilities p:\", p)\n",
    "\n",
    "# Cross-Entropy Loss\n",
    "true_context_index = word_to_index[context_word]\n",
    "loss = -np.log(p[true_context_index])\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# Error\n",
    "error = p.copy()\n",
    "error[true_context_index] -= 1\n",
    "print(\"Error vector:\", error)\n",
    "\n",
    "# Gradients\n",
    "grad_W2 = np.outer(h, error)\n",
    "grad_W1 = W2 @ error\n",
    "print(\"Gradient W2:\\n\", grad_W2)\n",
    "print(\"Gradient W1:\", grad_W1)\n",
    "\n",
    "# Update\n",
    "learning_rate = 0.1\n",
    "W2 -= learning_rate * grad_W2\n",
    "W1[word_to_index[center_word]] -= learning_rate * grad_W1\n",
    "\n",
    "print(\"\\nUpdated W1:\\n\", W1)\n",
    "print(\"Updated W2:\\n\", W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Python Code  \n",
    "**Idea:** Words that appear in similar contexts have similar embeddings (distributional hypothesis).  \n",
    "- **CBOW (Continuous Bag of Words):** Predicts a word from surrounding context.  \n",
    "- **Skip-gram:** Predicts surrounding context from a given word.\n",
    "\n",
    "**Example using `gensim`:**\n",
    "Gensim is a Python library for topic modeling and document similarity analysis. It provides efficient implementations of algorithms like Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA), and word2vec for discovering semantic structures in large text corpora.\n",
    "\n",
    "The role of Gensim in text analysis are as follows:\n",
    "\n",
    "- Text preprocessing: Gensim provides functions for preprocessing text data, including tokenization, normalization, stemming, and lemmatization, ensuring that the text is cleaned and standardized for further analysis.\n",
    "\n",
    "- Document Representation: Gensim allows users to represent documents as vectors in a high-dimensional space, facilitating various text analysis tasks such as document clustering, classification, and similarity analysis.\n",
    "\n",
    "- Word Embeddings: Gensim includes implementations of the word2vec, GloVe algorithm, which learns distributed representations of words in a vector space, capturing semantic relationships and similarities between words, facilitating tasks such as semantic similarity calculation, word analogy reasoning, and language understanding.\n",
    "\n",
    "- Topic Modeling: Gensim includes implementations of algorithms such as Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF) for topic modeling, enabling users to discover underlying topics within large text corpora.\n",
    "Document Similarity and Retrieval: Gensim provides functionality for computing similarities between documents based on their content, facilitating tasks such as document clustering, similarity analysis, and information retrieval.\n",
    "\n",
    "Overall, Gensim is a powerful library for discovering semantic structures in text data, offering efficient implementations of Text preprocessing,Document Representation, Word Embeddings, topic modeling, document similarity and Retrieval:. Its scalability and ease of use make it a popular choice for researchers and practitioners working with large text corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.3 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/moka/Library/Python/3.9/lib/python/site-packages (from gensim) (1.13.1)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.17.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Downloading gensim-4.3.3-cp39-cp39-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.17.2-cp39-cp39-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 smart-open-7.1.0 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moka/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'machine':\n",
      " [-0.01648536  0.01859871 -0.00039532 -0.00393455  0.00920726 -0.00819063\n",
      "  0.00548623  0.01387993  0.01213085 -0.01502159  0.0187647   0.00934362\n",
      "  0.00793224 -0.01248701  0.01691996 -0.00430033  0.01765038 -0.01072401\n",
      " -0.01625884  0.01364912  0.00334239 -0.00439702  0.0190272   0.01898771\n",
      " -0.01954809  0.00501046  0.01231338  0.00774491  0.00404557  0.000861\n",
      "  0.00134726 -0.00764127 -0.0142805  -0.00417774  0.0078478   0.01763737\n",
      "  0.0185183  -0.01195187 -0.01880534  0.01952875  0.00685957  0.01033223\n",
      "  0.01256469 -0.00560853  0.01464541  0.00566054  0.00574201 -0.00476074\n",
      " -0.0062565  -0.00474028]\n",
      "Similarity between 'machine' and 'learning': 0.11255005\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec  # Import Word2Vec\n",
    "from nltk.tokenize import word_tokenize  # Tokenizer for splitting sentences\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    \"Natural language processing is fun\",\n",
    "    \"Machine learning is a part of artificial intelligence\",\n",
    "    \"Word embeddings capture semantic meaning\"\n",
    "]\n",
    "\n",
    "# Tokenize the corpus (split each sentence into words)\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train the Word2Vec model using Skip-gram (sg=1) with vector size of 50\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=50, window=2, min_count=1, sg=1)\n",
    "\n",
    "# Get the embedding vector for the word 'machine'\n",
    "print(\"Embedding for 'machine':\\n\", model.wv['machine'])\n",
    "\n",
    "# Check similarity between two words\n",
    "print(\"Similarity between 'machine' and 'learning':\", model.wv.similarity('machine', 'learning'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ GloVe Embeddings\n",
    "\n",
    "Idea: Uses global word co-occurrence statistics across the entire corpus instead of local context windows.\n",
    "\n",
    "Example: Using Pretrained GloVe Embeddings (Common Crawl or Wikipedia):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embedding for 'machine':\n",
      " [-0.34165   -0.81267    1.4513     0.05914   -0.080801   0.39567\n",
      "  0.10064   -0.5468    -0.18887    0.11364   -0.040956  -0.5637\n",
      " -0.32191    0.15968   -0.59756   -0.14571   -0.77074    1.2955\n",
      " -0.72002   -0.90818    0.76644    0.05346   -0.0031632 -0.15341\n",
      "  0.22065   -1.191     -1.0775    -0.29768    1.327     -0.51359\n",
      "  2.6229    -0.67411   -0.82558    0.14283   -0.014214   0.90775\n",
      "  0.66828    0.48431    0.1543     0.26044    1.0191     0.015872\n",
      " -0.75325    0.58992    0.4546    -0.19678    0.42138   -0.43168\n",
      "  0.11985    0.14094  ]\n"
     ]
    }
   ],
   "source": [
    "# Download pretrained GloVe from: https://nlp.stanford.edu/projects/glove/\n",
    "# Example: 'glove.6B.50d.txt' contains 50-dimensional vectors\n",
    "# the word 'to' in txt file has the value as below : \n",
    "# to = [0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 .... -0.064699 -0.26044] with the length of 50\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load the GloVe embeddings (assuming the file 'glove.6B.50d.txt' is downloaded)\n",
    "glove_embeddings = {}\n",
    "with open('data/glove.6B.50d.txt', 'r', encoding='utf-8') as f: # text file saved in 'data' folder\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "# Example: Get embedding for 'machine'\n",
    "print(\"GloVe embedding for 'machine':\\n\", glove_embeddings.get('machine'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ FastText Embeddings\n",
    "\n",
    "Idea: Considers subword information (character n-grams), helping handle out-of-vocabulary (OOV) words better than Word2Vec and GloVe.\n",
    "\n",
    "FastText can generate embeddings for unseen words based on their subword components.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText embedding for 'machine':\n",
      " [-1.8079143e-03  2.1979981e-03 -1.7691230e-03  5.6677201e-04\n",
      "  1.2654356e-03  2.1495651e-03 -1.5551907e-03  4.3800026e-03\n",
      " -2.0496619e-03  5.3299527e-04 -2.5257603e-03  1.6439921e-03\n",
      "  3.4959912e-03 -9.7273325e-05  2.1534071e-03  2.0341277e-03\n",
      " -5.6405651e-04  1.8103337e-03  4.1687866e-03  7.4633321e-04\n",
      " -3.0441333e-03 -3.0280279e-03  3.9849104e-03 -7.3370530e-04\n",
      " -1.7331528e-03  1.6396311e-03 -6.8702095e-04 -2.2539324e-03\n",
      "  5.6145241e-04  1.4721482e-03 -2.8888162e-03 -2.2243629e-03\n",
      "  2.1639713e-03 -1.2766268e-03  6.0394765e-03  4.9851830e-03\n",
      "  3.3022531e-03  1.5956949e-03 -4.3668048e-03  1.5206628e-03\n",
      " -2.3396676e-03  7.1912521e-04  2.4290388e-03 -5.5817286e-03\n",
      "  2.9966333e-03 -6.4665275e-03 -5.4450257e-04 -2.2184665e-03\n",
      "  1.3568229e-03 -5.2718865e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText  # Import FastText\n",
    "\n",
    "# Use the same tokenized corpus\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train FastText model\n",
    "fasttext_model = FastText(sentences=tokenized_sentences, vector_size=50, window=2, min_count=1)\n",
    "\n",
    "# Get vector for a word\n",
    "print(\"FastText embedding for 'machine':\\n\", fasttext_model.wv['machine'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Why Use Word Embeddings?\n",
    "\n",
    "| Method          | Sparse/Dense    | Captures Word Meaning? | Handles OOV Words?      |\n",
    "|-----------------|-----------------|-----------------------|------------------------|\n",
    "| BoW / TF-IDF    | Sparse           | ‚ùå No                  | ‚ùå No                   |\n",
    "| Word2Vec        | Dense            | ‚úÖ Yes                 | ‚ùå No                   |\n",
    "| GloVe           | Dense            | ‚úÖ Yes                 | ‚ùå No                   |\n",
    "| FastText        | Dense            | ‚úÖ Yes                 | ‚úÖ Yes (via subwords)   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Answers to Practice Questions (Word Embeddings)\n",
    "\n",
    "### 1Ô∏è‚É£ Why are word embeddings better than BoW or TF-IDF for capturing meaning?\n",
    "Word embeddings (like Word2Vec, GloVe, FastText) map words into dense vectors where **semantically similar words are closer together in the vector space**. Unlike BoW or TF-IDF, which only count word occurrences and ignore word order or meaning, embeddings capture relationships between words (e.g., \"king\" is close to \"queen\", \"Paris\" is close to \"France\").\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ What is the main difference between Skip-gram and CBOW in Word2Vec?\n",
    "- **CBOW (Continuous Bag of Words):** Predicts the target word based on its surrounding context words.\n",
    "- **Skip-gram:** Predicts the surrounding context words given the target word.\n",
    "- Typically, **Skip-gram works better for small datasets** and rare words, while **CBOW is faster on large datasets**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ How does FastText handle words that it has not seen during training?\n",
    "FastText breaks words into **subword units (character n-grams)**. This allows it to create word vectors by combining the vectors of these subwords. Even if a word was not in the training data (out-of-vocabulary, OOV), FastText can generate a vector based on its subword pieces, making it more robust to rare or unseen words.\n",
    "\n",
    "Example:  \n",
    "The word **\"running\"** may be broken into subwords like `\"run\"`, `\"unn\"`, `\"nni\"`, `\"nin\"`, `\"ing\"`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Python Full Step-by-Step Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial W1:\n",
      " [[ 0.49671415 -0.1382643 ]\n",
      " [ 0.64768854  1.52302986]\n",
      " [-0.23415337 -0.23413696]\n",
      " [ 1.57921282  0.76743473]]\n",
      "Initial W2:\n",
      " [[-0.46947439  0.54256004 -0.46341769 -0.46572975]\n",
      " [ 0.24196227 -1.91328024 -1.72491783 -0.56228753]]\n",
      "\n",
      "Hidden vector h (cat): [-0.23415337 -0.23413696]\n",
      "Output logits u: [0.0532767  0.32092735 0.51237783 0.24070448]\n",
      "Predicted probabilities p: [0.19624449 0.25647006 0.31058649 0.23669895]\n",
      "Loss: 1.4409661987967817\n",
      "\n",
      "Updated W1:\n",
      " [[ 0.49671415 -0.1382643 ]\n",
      " [ 0.64768854  1.52302986]\n",
      " [-0.26001131 -0.17916127]\n",
      " [ 1.57921282  0.76743473]]\n",
      "Updated W2:\n",
      " [[-0.46487925  0.54856538 -0.45614521 -0.48360271]\n",
      " [ 0.24655708 -1.90727533 -1.71764585 -0.58015923]]\n",
      "\n",
      "Hidden vector h (dog): [1.57921282 0.76743473]\n",
      "Output logits u: [-0.54492681 -0.59740786 -2.03853144 -1.20894593]\n",
      "Predicted probabilities p: [0.37199444 0.35297522 0.08353572 0.19149463]\n",
      "Loss: 0.9888763844670431\n",
      "\n",
      "Updated W1 after 2nd iteration:\n",
      " [[ 0.49671415 -0.1382643 ]\n",
      " [ 0.64768854  1.52302986]\n",
      " [-0.26001131 -0.17916127]\n",
      " [ 1.54372632  0.87569896]]\n",
      "Updated W2 after 2nd iteration:\n",
      " [[-0.36570381  0.49282308 -0.46933727 -0.51384378]\n",
      " [ 0.29475241 -1.93436388 -1.72405668 -0.59485519]]\n"
     ]
    }
   ],
   "source": [
    "# 1. What is Word2Vec?\n",
    "\n",
    "# Word2Vec learns a vector (embedding) for each word so that similar words are close together in vector space.\n",
    "# Two main models:\n",
    "# - CBOW: Predict center word from context words\n",
    "# - Skip-Gram: Predict context words from center word\n",
    "\n",
    "# 2. Key Components\n",
    "\n",
    "# | Component      | Meaning                                              |\n",
    "# |----------------|------------------------------------------------------|\n",
    "# | One-hot vector | Input where only one element is 1, others are 0       |\n",
    "# | W1 (input)     | (V x d) matrix: one-hot ‚Üí hidden dense vector         |\n",
    "# | W2 (output)    | (d x V) matrix: hidden vector ‚Üí output vocabulary     |\n",
    "\n",
    "# 3. Mathematical Flow\n",
    "\n",
    "# Given:\n",
    "# - Vocabulary size = V\n",
    "# - Embedding size = d\n",
    "\n",
    "# Step-by-Step Computation:\n",
    "# 1. Input: One-hot vector x (size V)\n",
    "# 2. Hidden Layer: h = W1^T x\n",
    "# 3. Output Layer: u = W2^T h\n",
    "# 4. Softmax: p = softmax(u)\n",
    "# 5. Loss: Cross-entropy\n",
    "# 6. Update W1 and W2\n",
    "\n",
    "# 4. Example Setup\n",
    "\n",
    "# Vocabulary = [\"apple\", \"banana\", \"cat\", \"dog\"]\n",
    "# V = 4, d = 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Vocabulary\n",
    "vocab = [\"apple\", \"banana\", \"cat\", \"dog\"]\n",
    "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "V = len(vocab)\n",
    "d = 2\n",
    "\n",
    "# Initial weight matrices\n",
    "W1 = np.random.randn(V, d)\n",
    "W2 = np.random.randn(d, V)\n",
    "\n",
    "print(\"Initial W1:\\n\", W1)\n",
    "print(\"Initial W2:\\n\", W2)\n",
    "\n",
    "# 5. Iteration 1: Center word \"cat\", Context word \"dog\"\n",
    "\n",
    "center_word = \"cat\"\n",
    "context_word = \"dog\"\n",
    "\n",
    "# One-hot vector for \"cat\"\n",
    "x = np.zeros(V)\n",
    "x[word_to_index[center_word]] = 1\n",
    "\n",
    "# Step 1: Hidden Layer\n",
    "h = W1.T @ x  # or simply W1[cat]\n",
    "print(\"\\nHidden vector h (cat):\", h)\n",
    "\n",
    "# Step 2: Output Layer\n",
    "u = W2.T @ h\n",
    "print(\"Output logits u:\", u)\n",
    "\n",
    "# Step 3: Softmax\n",
    "\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z))  # for numerical stability\n",
    "    return e_z / e_z.sum()\n",
    "\n",
    "p = softmax(u)\n",
    "print(\"Predicted probabilities p:\", p)\n",
    "\n",
    "# Step 4: Compute Loss\n",
    "true_context_index = word_to_index[context_word]\n",
    "loss = -np.log(p[true_context_index])\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# Step 5: Backpropagation (Simple Gradient Update)\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Gradients\n",
    "error = p.copy()\n",
    "error[true_context_index] -= 1  # p - y (one-hot)\n",
    "\n",
    "# Gradients w.r.t W2 and W1\n",
    "grad_W2 = np.outer(h, error)\n",
    "grad_W1 = W2 @ error\n",
    "\n",
    "# Update W1 and W2\n",
    "W1[word_to_index[center_word]] -= learning_rate * grad_W1\n",
    "W2 -= learning_rate * grad_W2\n",
    "\n",
    "print(\"\\nUpdated W1:\\n\", W1)\n",
    "print(\"Updated W2:\\n\", W2)\n",
    "\n",
    "# 6. Iteration 2: Center word \"dog\", Context word \"barks\"\n",
    "\n",
    "# For this example, assume \"barks\" is treated as a known word (for simplicity).\n",
    "# Here, let's use \"apple\" as the pretend context word.\n",
    "center_word = \"dog\"\n",
    "context_word = \"apple\"\n",
    "\n",
    "# One-hot vector for \"dog\"\n",
    "x = np.zeros(V)\n",
    "x[word_to_index[center_word]] = 1\n",
    "\n",
    "# Step 1: Hidden Layer\n",
    "h = W1.T @ x\n",
    "print(\"\\nHidden vector h (dog):\", h)\n",
    "\n",
    "# Step 2: Output Layer\n",
    "u = W2.T @ h\n",
    "print(\"Output logits u:\", u)\n",
    "\n",
    "# Step 3: Softmax\n",
    "p = softmax(u)\n",
    "print(\"Predicted probabilities p:\", p)\n",
    "\n",
    "# Step 4: Compute Loss\n",
    "true_context_index = word_to_index[context_word]\n",
    "loss = -np.log(p[true_context_index])\n",
    "print(\"Loss:\", loss)\n",
    "\n",
    "# Step 5: Backpropagation\n",
    "error = p.copy()\n",
    "error[true_context_index] -= 1\n",
    "\n",
    "grad_W2 = np.outer(h, error)\n",
    "grad_W1 = W2 @ error\n",
    "\n",
    "# Update W1 and W2\n",
    "W1[word_to_index[center_word]] -= learning_rate * grad_W1\n",
    "W2 -= learning_rate * grad_W2\n",
    "\n",
    "print(\"\\nUpdated W1 after 2nd iteration:\\n\", W1)\n",
    "print(\"Updated W2 after 2nd iteration:\\n\", W2)\n",
    "\n",
    "### üî• Summary\n",
    "\n",
    "# - W1 maps one-hot vectors to dense hidden vectors (word embeddings).\n",
    "# - W2 maps hidden vectors back to vocabulary.\n",
    "# - Both W1 and W2 are updated every iteration to reduce prediction error.\n",
    "# - After many iterations, W1 contains meaningful word embeddings!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
