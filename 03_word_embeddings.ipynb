{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåê Chapter 3: Word Embeddings\n",
    "\n",
    "## üìå Overview  \n",
    "(Recall) Bag of Words (BoW) and TF-IDF represent words as sparse vectors where each dimension corresponds to a unique word. However, they fail to capture **semantic relationships** between words (e.g., \"king\" and \"queen\").\n",
    "\n",
    "**Word Embeddings** solve this problem by mapping words into dense, low-dimensional vectors where similar words have similar representations. These embeddings are learned from large text corpora.\n",
    "\n",
    "Popular embedding methods:\n",
    "- Word2Vec (Skip-gram, CBOW)\n",
    "- GloVe (Global Vectors)\n",
    "- FastText\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üöÄ Word2Vec (Skip-gram with Negative Sampling)\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Vocabulary and Indexing\n",
    "\n",
    "Given corpus: `\"The cat sits\"`  \n",
    "Vocabulary size $V = 3$.\n",
    "\n",
    "| Word   | Index |\n",
    "|--------|-------|\n",
    "| the    | 0     |\n",
    "| cat    | 1     |\n",
    "| sits   | 2     |\n",
    "\n",
    "---\n",
    "\n",
    "### üü° Step 1: Input Embedding Matrix $W$ (Center Words)\n",
    "\n",
    "- Shape: $V \\times d = 3 \\times 2$\n",
    "- Each **row** represents the embedding vector for a word **when it is the center word**.\n",
    "\n",
    "$$\n",
    "W =\n",
    "\\begin{bmatrix}\n",
    "0.2 & -0.1 \\\\\n",
    "0.7 & 0.3 \\\\\n",
    "-0.5 & 0.6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- $u_{\\text{the}} = [0.2, -0.1]$\n",
    "- $u_{\\text{cat}} = [0.7, 0.3]$\n",
    "- $u_{\\text{sits}} = [-0.5, 0.6]$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Mathematical Concept:**\n",
    "The input embedding matrix $W$ maps one-hot encoded center words to dense embeddings.  \n",
    "For a one-hot vector $x_{w_t}$ for center word $w_t$:\n",
    "\n",
    "$$\n",
    "u_{w_t} = W^\\top x_{w_t}\n",
    "$$\n",
    "\n",
    "This operation selects the $w_t$-th row of $W$ directly.\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Step 2: Output Embedding Matrix $W'$ (Context Words, different from the input weights W)\n",
    "\n",
    "- Shape: $V \\times d = 3 \\times 2$\n",
    "- Each **row** represents the embedding vector for a word **when it is a context word**.\n",
    "\n",
    "$$\n",
    "W' =\n",
    "\\begin{bmatrix}\n",
    "0.1 & 0.4 \\\\\n",
    "-0.3 & 0.5 \\\\\n",
    "0.6 & -0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- $v_{\\text{the}} = [0.1, 0.4]$\n",
    "- $v_{\\text{cat}} = [-0.3, 0.5]$\n",
    "- $v_{\\text{sits}} = [0.6, -0.2]$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Mathematical Concept:**\n",
    "The output embedding matrix $W'$ holds context word vectors.  \n",
    "During prediction, the center word's embedding (from $W$) is compared with these context vectors (from $W'$) via dot product.\n",
    "\n",
    "---\n",
    "\n",
    "### üü£ Step 3: Dot Product Calculation (Score Computation)\n",
    "\n",
    "Center word = `\"cat\"` (index 1):\n",
    "\n",
    "$$\n",
    "u_{\\text{cat}} = [0.7, 0.3]\n",
    "$$\n",
    "\n",
    "The score between center word $u_{\\text{cat}}$ and each context word $v_{w_o}$ is:\n",
    "\n",
    "$$\n",
    "s_{w_o} = u_{\\text{cat}}^\\top v_{w_o}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Detailed Calculations:**\n",
    "\n",
    "1. Between `\"cat\"` (center) and `\"the\"` (context):\n",
    "\n",
    "$$\n",
    "s_{\\text{the}} = (0.7 \\times 0.1) + (0.3 \\times 0.4) = 0.07 + 0.12 = 0.19\n",
    "$$\n",
    "\n",
    "2. Between `\"cat\"` (center) and `\"cat\"` (context):\n",
    "\n",
    "$$\n",
    "s_{\\text{cat}} = (0.7 \\times -0.3) + (0.3 \\times 0.5) = -0.21 + 0.15 = -0.06\n",
    "$$\n",
    "\n",
    "3. Between `\"cat\"` (center) and `\"sits\"` (context):\n",
    "\n",
    "$$\n",
    "s_{\\text{sits}} = (0.7 \\times 0.6) + (0.3 \\times -0.2) = 0.42 - 0.06 = 0.36\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Step 4: Resulting Raw Scores Table\n",
    "\n",
    "| Center Word | Context Word | Dot Product Score |\n",
    "|-------------|--------------|------------------|\n",
    "| cat         | the          | 0.19             |\n",
    "| cat         | cat          | -0.06            |\n",
    "| cat         | sits         | 0.36             |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° Mathematical Concept Behind Dot Product Scoring\n",
    "\n",
    "The dot product measures **alignment** or **similarity** between the embedding vectors of the center and context words:\n",
    "\n",
    "$$\n",
    "s_{w_o} = \\sum_{i=1}^{d} u_{w_t}^{(i)} \\cdot v_{w_o}^{(i)}\n",
    "$$\n",
    "\n",
    "- Large positive dot product ‚Üí vectors point in a similar direction (high similarity).\n",
    "- Zero ‚Üí vectors are orthogonal (unrelated).\n",
    "- Negative ‚Üí vectors point in opposite directions (dissimilar).\n",
    "\n",
    "These scores are:\n",
    "- Passed into the **softmax function** to obtain probabilities:\n",
    "\n",
    "$$\n",
    "p(w_o \\mid w_t) = \\frac{\\exp(s_{w_o})}{\\sum_{w=1}^{V} \\exp(s_w)}\n",
    "$$\n",
    "\n",
    "- Or used directly for **negative sampling loss** in more efficient training.\n",
    "\n",
    "#### Apply Softmax to Get Probabilities\n",
    "\n",
    "Softmax formula:\n",
    "\n",
    "$$\n",
    "p(w_o \\mid w_t) = \\frac{\\exp(s_{w_o})}{\\sum_{w=1}^{V} \\exp(s_{w})}\n",
    "$$\n",
    "\n",
    "Exponentials:\n",
    "- $\\exp(0.19) \\approx 1.209$\n",
    "- $\\exp(-0.06) \\approx 0.942$\n",
    "- $\\exp(0.36) \\approx 1.433$\n",
    "\n",
    "Denominator:\n",
    "\n",
    "$$\n",
    "1.209 + 0.942 + 1.433 = 3.584\n",
    "$$\n",
    "\n",
    "Probabilities:\n",
    "- $p(\\text{the} \\mid \\text{cat}) \\approx 0.337$\n",
    "- $p(\\text{cat} \\mid \\text{cat}) \\approx 0.263$\n",
    "- $p(\\text{sits} \\mid \\text{cat}) \\approx 0.400$\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Step 5: Calculate Loss (True Context is \"the\")\n",
    "\n",
    "Loss:\n",
    "\n",
    "$$\n",
    "L = -\\log p(\\text{the} \\mid \\text{cat})\n",
    "$$\n",
    "\n",
    "Substituting:\n",
    "\n",
    "$$\n",
    "L = -\\log(0.337) \\approx 1.087\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Step 6: Negative Sampling Alternative (Optional)\n",
    "\n",
    "If using negative sampling:\n",
    "- Positive pair: (\"cat\", \"the\")\n",
    "- Negative pairs: (\"cat\", \"cat\") and (\"cat\", \"sits\")\n",
    "\n",
    "Loss:\n",
    "\n",
    "$$\n",
    "L = -\\log \\sigma(u_{\\text{cat}}^\\top v_{\\text{the}}) - \\sum_{i=1}^{k} \\log \\sigma(-u_{\\text{cat}}^\\top v_{\\text{neg}, i})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Conclusion\n",
    "\n",
    "- The model maximizes dot product scores for correct pairs.\n",
    "- Reduces scores for incorrect pairs.\n",
    "- Learns meaningful word embeddings where similar words have similar vector representations.\n",
    "\n",
    "üß† Why This Works:\n",
    "\n",
    "- The model **encourages higher scores** for true (center, context) pairs.\n",
    "- It **reduces scores** for randomly sampled negative (incorrect) pairs.\n",
    "- Over time, embeddings for words appearing together in similar contexts **move closer in the vector space**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1Ô∏è‚É£ Introduction to Word2Vec  \n",
    "**Idea:** Words that appear in similar contexts have similar embeddings (distributional hypothesis).  \n",
    "- **CBOW (Continuous Bag of Words):** Predicts a word from surrounding context.  \n",
    "- **Skip-gram:** Predicts surrounding context from a given word.\n",
    "\n",
    "**Example using `gensim`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.3 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/moka/Library/Python/3.9/lib/python/site-packages (from gensim) (1.13.1)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.17.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Downloading gensim-4.3.3-cp39-cp39-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.17.2-cp39-cp39-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 smart-open-7.1.0 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moka/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'machine':\n",
      " [-0.01648536  0.01859871 -0.00039532 -0.00393455  0.00920726 -0.00819063\n",
      "  0.00548623  0.01387993  0.01213085 -0.01502159  0.0187647   0.00934362\n",
      "  0.00793224 -0.01248701  0.01691996 -0.00430033  0.01765038 -0.01072401\n",
      " -0.01625884  0.01364912  0.00334239 -0.00439702  0.0190272   0.01898771\n",
      " -0.01954809  0.00501046  0.01231338  0.00774491  0.00404557  0.000861\n",
      "  0.00134726 -0.00764127 -0.0142805  -0.00417774  0.0078478   0.01763737\n",
      "  0.0185183  -0.01195187 -0.01880534  0.01952875  0.00685957  0.01033223\n",
      "  0.01256469 -0.00560853  0.01464541  0.00566054  0.00574201 -0.00476074\n",
      " -0.0062565  -0.00474028]\n",
      "Similarity between 'machine' and 'learning': 0.11255005\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec  # Import Word2Vec\n",
    "from nltk.tokenize import word_tokenize  # Tokenizer for splitting sentences\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    \"Natural language processing is fun\",\n",
    "    \"Machine learning is a part of artificial intelligence\",\n",
    "    \"Word embeddings capture semantic meaning\"\n",
    "]\n",
    "\n",
    "# Tokenize the corpus (split each sentence into words)\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train the Word2Vec model using Skip-gram (sg=1) with vector size of 50\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=50, window=2, min_count=1, sg=1)\n",
    "\n",
    "# Get the embedding vector for the word 'machine'\n",
    "print(\"Embedding for 'machine':\\n\", model.wv['machine'])\n",
    "\n",
    "# Check similarity between two words\n",
    "print(\"Similarity between 'machine' and 'learning':\", model.wv.similarity('machine', 'learning'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ GloVe Embeddings\n",
    "\n",
    "Idea: Uses global word co-occurrence statistics across the entire corpus instead of local context windows.\n",
    "\n",
    "Example: Using Pretrained GloVe Embeddings (Common Crawl or Wikipedia):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embedding for 'machine':\n",
      " [-0.34165   -0.81267    1.4513     0.05914   -0.080801   0.39567\n",
      "  0.10064   -0.5468    -0.18887    0.11364   -0.040956  -0.5637\n",
      " -0.32191    0.15968   -0.59756   -0.14571   -0.77074    1.2955\n",
      " -0.72002   -0.90818    0.76644    0.05346   -0.0031632 -0.15341\n",
      "  0.22065   -1.191     -1.0775    -0.29768    1.327     -0.51359\n",
      "  2.6229    -0.67411   -0.82558    0.14283   -0.014214   0.90775\n",
      "  0.66828    0.48431    0.1543     0.26044    1.0191     0.015872\n",
      " -0.75325    0.58992    0.4546    -0.19678    0.42138   -0.43168\n",
      "  0.11985    0.14094  ]\n"
     ]
    }
   ],
   "source": [
    "# Download pretrained GloVe from: https://nlp.stanford.edu/projects/glove/\n",
    "# Example: 'glove.6B.50d.txt' contains 50-dimensional vectors\n",
    "# the word 'to' in txt file has the value as below : \n",
    "# to = [0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 .... -0.064699 -0.26044] with the length of 50\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load the GloVe embeddings (assuming the file 'glove.6B.50d.txt' is downloaded)\n",
    "glove_embeddings = {}\n",
    "with open('data/glove.6B.50d.txt', 'r', encoding='utf-8') as f: # text file saved in 'data' folder\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "# Example: Get embedding for 'machine'\n",
    "print(\"GloVe embedding for 'machine':\\n\", glove_embeddings.get('machine'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ FastText Embeddings\n",
    "\n",
    "Idea: Considers subword information (character n-grams), helping handle out-of-vocabulary (OOV) words better than Word2Vec and GloVe.\n",
    "\n",
    "FastText can generate embeddings for unseen words based on their subword components.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText embedding for 'machine':\n",
      " [-1.8079143e-03  2.1979981e-03 -1.7691230e-03  5.6677201e-04\n",
      "  1.2654356e-03  2.1495651e-03 -1.5551907e-03  4.3800026e-03\n",
      " -2.0496619e-03  5.3299527e-04 -2.5257603e-03  1.6439921e-03\n",
      "  3.4959912e-03 -9.7273325e-05  2.1534071e-03  2.0341277e-03\n",
      " -5.6405651e-04  1.8103337e-03  4.1687866e-03  7.4633321e-04\n",
      " -3.0441333e-03 -3.0280279e-03  3.9849104e-03 -7.3370530e-04\n",
      " -1.7331528e-03  1.6396311e-03 -6.8702095e-04 -2.2539324e-03\n",
      "  5.6145241e-04  1.4721482e-03 -2.8888162e-03 -2.2243629e-03\n",
      "  2.1639713e-03 -1.2766268e-03  6.0394765e-03  4.9851830e-03\n",
      "  3.3022531e-03  1.5956949e-03 -4.3668048e-03  1.5206628e-03\n",
      " -2.3396676e-03  7.1912521e-04  2.4290388e-03 -5.5817286e-03\n",
      "  2.9966333e-03 -6.4665275e-03 -5.4450257e-04 -2.2184665e-03\n",
      "  1.3568229e-03 -5.2718865e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText  # Import FastText\n",
    "\n",
    "# Use the same tokenized corpus\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train FastText model\n",
    "fasttext_model = FastText(sentences=tokenized_sentences, vector_size=50, window=2, min_count=1)\n",
    "\n",
    "# Get vector for a word\n",
    "print(\"FastText embedding for 'machine':\\n\", fasttext_model.wv['machine'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Why Use Word Embeddings?\n",
    "\n",
    "| Method          | Sparse/Dense    | Captures Word Meaning? | Handles OOV Words?      |\n",
    "|-----------------|-----------------|-----------------------|------------------------|\n",
    "| BoW / TF-IDF    | Sparse           | ‚ùå No                  | ‚ùå No                   |\n",
    "| Word2Vec        | Dense            | ‚úÖ Yes                 | ‚ùå No                   |\n",
    "| GloVe           | Dense            | ‚úÖ Yes                 | ‚ùå No                   |\n",
    "| FastText        | Dense            | ‚úÖ Yes                 | ‚úÖ Yes (via subwords)   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Answers to Practice Questions (Word Embeddings)\n",
    "\n",
    "### 1Ô∏è‚É£ Why are word embeddings better than BoW or TF-IDF for capturing meaning?\n",
    "Word embeddings (like Word2Vec, GloVe, FastText) map words into dense vectors where **semantically similar words are closer together in the vector space**. Unlike BoW or TF-IDF, which only count word occurrences and ignore word order or meaning, embeddings capture relationships between words (e.g., \"king\" is close to \"queen\", \"Paris\" is close to \"France\").\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ What is the main difference between Skip-gram and CBOW in Word2Vec?\n",
    "- **CBOW (Continuous Bag of Words):** Predicts the target word based on its surrounding context words.\n",
    "- **Skip-gram:** Predicts the surrounding context words given the target word.\n",
    "- Typically, **Skip-gram works better for small datasets** and rare words, while **CBOW is faster on large datasets**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ How does FastText handle words that it has not seen during training?\n",
    "FastText breaks words into **subword units (character n-grams)**. This allows it to create word vectors by combining the vectors of these subwords. Even if a word was not in the training data (out-of-vocabulary, OOV), FastText can generate a vector based on its subword pieces, making it more robust to rare or unseen words.\n",
    "\n",
    "Example:  \n",
    "The word **\"running\"** may be broken into subwords like `\"run\"`, `\"unn\"`, `\"nni\"`, `\"nin\"`, `\"ing\"`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
