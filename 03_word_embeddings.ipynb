{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåê Chapter 3: Word Embeddings\n",
    "\n",
    "## üìå Overview  \n",
    "(Recall) Bag of Words (BoW) and TF-IDF represent words as sparse vectors where each dimension corresponds to a unique word. However, they fail to capture **semantic relationships** between words (e.g., \"king\" and \"queen\").\n",
    "\n",
    "**Word Embeddings** solve this problem by mapping words into dense, low-dimensional vectors where similar words have similar representations. These embeddings are learned from large text corpora.\n",
    "\n",
    "Popular embedding methods:\n",
    "- Word2Vec (Skip-gram, CBOW)\n",
    "- GloVe (Global Vectors)\n",
    "- FastText\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Word2Vec Works (Skip-gram with Negative Sampling)\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ Step 1: Define Corpus, Vocabulary, and Input Representation\n",
    "\n",
    "- **Example sentence**:  \n",
    "  `\"The cat sits\"`\n",
    "\n",
    "- **Vocabulary (V = 3)**:  \n",
    "  `{the, cat, sits}`\n",
    "\n",
    "  | Word   | Index |\n",
    "  |--------|-------|\n",
    "  | the    | 0     |\n",
    "  | cat    | 1     |\n",
    "  | sits   | 2     |\n",
    "\n",
    "  where the vocabulary size $V = 3$.\n",
    "\n",
    "- One-Hot Encoding (for input):\n",
    "  For \"cat\" (index 1):\n",
    "\n",
    "  $$\n",
    "  x_{\tthe} =\n",
    "  \\begin{bmatrix}\n",
    "  1 \\\\\n",
    "  0 \\\\\n",
    "  0\n",
    "  \\end{bmatrix},\n",
    "  \\;\n",
    "  x_{\tcat} =\n",
    "  \\begin{bmatrix}\n",
    "  0 \\\\\n",
    "  1 \\\\\n",
    "  0\n",
    "  \\end{bmatrix},\n",
    "  \\;\n",
    "  x_{\tsits} =\n",
    "  \\begin{bmatrix}\n",
    "  0 \\\\\n",
    "  0 \\\\\n",
    "  1\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "\n",
    "- Shape of each $ x $: \n",
    "$ V \\times 1 = 3 \\times 1$\n",
    "\n",
    "---\n",
    "\n",
    "### üü° Step 2: Network Architecture (Embedding Layer)\n",
    "\n",
    "- **Input weight matrix \\( W \\)**:\n",
    "- Shape: $ V \\times d $ wehre $d$ is embedding dimension (parameter).\n",
    "- Each row of $ W $ is the embedding for one word.\n",
    "\n",
    "- **Example when $ d = 3 $**:\n",
    "$$\n",
    "W = \n",
    "\\begin{bmatrix}\n",
    "0.2 & -0.3 & 0.1 \\quad\\text{‚Üê embedding of \"the\"}\\\\\n",
    "0.5 & 0.1  & -0.4 \\quad\\text{‚Üê embedding of \"cat\"}\\\\\n",
    "\\vdots & \\vdots & \\vdots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- **Hidden layer output (embedding lookup)**:\n",
    "$$\n",
    "h = W^\\top x \\Rightarrow W^\\top X \\; \\text{for all x, matrix format}\n",
    "$$\n",
    "- Simply selects the row of $ W $ for the center word.\n",
    "- Shape of $ h $: $ d \\times 1 $\n",
    "- In this Example : \n",
    "\n",
    "  Input weight matrix $W$ (input embeddings):\n",
    "  Embedding dimension $d = 2$.  \n",
    "  Shape of $W$: $3 \\times 2$ (vocabulary size $V = 3$, embedding size $d = 2$).\n",
    "\n",
    "  $$\n",
    "  W =\n",
    "  \\begin{bmatrix}\n",
    "  0.2 & -0.1 \\\\\n",
    "  0.7 & 0.3 \\\\\n",
    "  -0.5 & 0.6\n",
    "  \\end{bmatrix}\n",
    "  \\Rightarrow\n",
    "  W^\\top =\n",
    "  \\begin{bmatrix}\n",
    "  0.2 & 0.7&-0.5 \\\\\n",
    "  -0.1 & 0.3 & 0.6\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "\n",
    "- Embedding Lookup (Hidden Layer Output):\n",
    "  For the input word \"cat\":\n",
    "\n",
    "  $$\n",
    "  h = W^\\top x_{\\text{cat}}\n",
    "  $$\n",
    "\n",
    "  Result (selects the 2nd row of $W$):\n",
    "\n",
    "  $$\n",
    "  h = u_{\\text{cat}} =\n",
    "  \\begin{bmatrix}\n",
    "  0.7 \\\\\n",
    "  0.3\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üü† Step 3: Compute Context Scores\n",
    "\n",
    "- **Output weight matrix $ W'$**:\n",
    "- Shape: $ d \\times V $\n",
    "- Each column of $ W'$ represents the output vector for one word.\n",
    "\n",
    "- **Score calculation** (dot product between center and context embeddings):\n",
    "$$\n",
    "s_{w_o} = u_{w_t}^\\top v_{w_o}\n",
    "$$\n",
    "- $ u_{w_t} $: input embedding of the center word.\n",
    "- $ v_{w_o} $: output embedding of the context word.\n",
    "- Output weight matrix $W'$ (context embeddings):\n",
    "  Shape of $W'$: $2 \\times 3$.\n",
    "\n",
    "  $$\n",
    "  W' =\n",
    "  \\begin{bmatrix}\n",
    "  0.1 & -0.2 & 0.4 \\\\\n",
    "  0.6 & 0.5 & -0.3\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  - Column 0 ‚Üí $v_{\\text{the}} = \\begin{bmatrix} 0.1 \\\\ 0.6 \\end{bmatrix}$\n",
    "  - Column 1 ‚Üí $v_{\\text{cat}} = \\begin{bmatrix} -0.2 \\\\ 0.5 \\end{bmatrix}$\n",
    "  - Column 2 ‚Üí $v_{\\text{sits}} = \\begin{bmatrix} 0.4 \\\\ -0.3 \\end{bmatrix}$\n",
    "\n",
    "- Dot Product Scores:\n",
    "  $$\n",
    "  s_{\\text{the}} = (0.7)(0.1) + (0.3)(0.6) = 0.25\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  s_{\\text{cat}} = (0.7)(-0.2) + (0.3)(0.5) = 0.01\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  s_{\\text{sits}} = (0.7)(0.4) + (0.3)(-0.3) = 0.19\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### üü£ Step 4: Probability via Softmax\n",
    "\n",
    "- **Softmax probability** of predicting context word $w_o $ given center word $w_t $:\n",
    "$$\n",
    "p(w_o \\mid w_t) = \\frac{\\exp(u_{w_t}^\\top v_{w_o})}{\\sum_{w=1}^{V} \\exp(u_{w_t}^\\top v_w)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üî¥ Step 5: Loss Function (Negative Log-Likelihood)\n",
    "\n",
    "- **Loss for one true pair $(w_t, w_o)$**:\n",
    "$$\n",
    "L = -\\log p(w_o \\mid w_t)\n",
    "$$\n",
    "- This loss encourages the embeddings of true word pairs to be close together.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° Step 6: Negative Sampling (Efficient Training)\n",
    "\n",
    "- **Why negative sampling?**\n",
    "- Computing full softmax is too slow when $ V $ is large.\n",
    "- Instead, train a **binary classifier**:\n",
    "  - Positive pair: $ (w_t, w_o) $ ‚Üí label 1.\n",
    "  - Negative pairs: $ (w_t, w_{\\text{neg}}) $ ‚Üí label 0.\n",
    "\n",
    "- **Negative sampling loss**:\n",
    "$$\n",
    "L = -\\log \\sigma(u_{w_t}^\\top v_{w_o}) - \\sum_{i=1}^{k} \\log \\sigma(-u_{w_t}^\\top v_{w_{\\text{neg}, i}})\n",
    "$$\n",
    "- $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $ is the sigmoid function.\n",
    "- Only involves true context and $ k $ negative samples.\n",
    "\n",
    "---\n",
    "\n",
    "### üü§ Step 7: Backpropagation and Updates\n",
    "\n",
    "- **Compute gradients w.r.t.:**\n",
    "- Center embedding $ u_{w_t} $\n",
    "- Output embedding $ v_{w_o} $\n",
    "- Negative samples $ v_{w_{\\text{neg}}} $\n",
    "\n",
    "- **Update embeddings using SGD or Adam**:\n",
    "$$\n",
    "u_{w_t} \\leftarrow u_{w_t} - \\eta \\frac{\\partial L}{\\partial u_{w_t}}\n",
    "$$\n",
    "$$\n",
    "v_{w_o} \\leftarrow v_{w_o} - \\eta \\frac{\\partial L}{\\partial v_{w_o}}\n",
    "$$\n",
    "$$\n",
    "v_{w_{\\text{neg}, i}} \\leftarrow v_{w_{\\text{neg}, i}} - \\eta \\frac{\\partial L}{\\partial v_{w_{\\text{neg}, i}}}\n",
    "$$\n",
    "- $ \\eta $: learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Step 8: Final Output ‚Äî Learned Word Embeddings\n",
    "\n",
    "- After training, **rows of $ W $** are the final learned word embeddings.\n",
    "- Similar words have embeddings that are close together in the vector space.\n",
    "\n",
    "---\n",
    "\n",
    "### üåü Summary of Key Variables\n",
    "\n",
    "| Symbol                | Meaning                                | Shape             |\n",
    "|------------------------|-----------------------------------------|-------------------|\n",
    "| $ x $                | One-hot vector of center word           | $ V \\times 1 $   |\n",
    "| $ W $                | Input weight matrix (embeddings)        | $ V \\times d $   |\n",
    "| $ W' $               | Output weight matrix (context vectors)  | $ d \\times V $   |\n",
    "| $ u_{w_t} $          | Embedding of center word                | $ d \\times 1 $   |\n",
    "| $ v_{w_o} $          | Embedding of context/output word        | $ d \\times 1 $   |\n",
    "| $ s_{w_o} $          | Score (dot product)                     | Scalar            |\n",
    "| $ p(w_o \\mid w_t) $  | Probability from softmax                | Scalar            |\n",
    "| $ L $                | Loss                                    | Scalar            |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1Ô∏è‚É£ Introduction to Word2Vec  \n",
    "**Idea:** Words that appear in similar contexts have similar embeddings (distributional hypothesis).  \n",
    "- **CBOW (Continuous Bag of Words):** Predicts a word from surrounding context.  \n",
    "- **Skip-gram:** Predicts surrounding context from a given word.\n",
    "\n",
    "**Example using `gensim`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.3 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/moka/Library/Python/3.9/lib/python/site-packages (from gensim) (1.13.1)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.17.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Downloading gensim-4.3.3-cp39-cp39-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.17.2-cp39-cp39-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 smart-open-7.1.0 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moka/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'machine':\n",
      " [-0.01648536  0.01859871 -0.00039532 -0.00393455  0.00920726 -0.00819063\n",
      "  0.00548623  0.01387993  0.01213085 -0.01502159  0.0187647   0.00934362\n",
      "  0.00793224 -0.01248701  0.01691996 -0.00430033  0.01765038 -0.01072401\n",
      " -0.01625884  0.01364912  0.00334239 -0.00439702  0.0190272   0.01898771\n",
      " -0.01954809  0.00501046  0.01231338  0.00774491  0.00404557  0.000861\n",
      "  0.00134726 -0.00764127 -0.0142805  -0.00417774  0.0078478   0.01763737\n",
      "  0.0185183  -0.01195187 -0.01880534  0.01952875  0.00685957  0.01033223\n",
      "  0.01256469 -0.00560853  0.01464541  0.00566054  0.00574201 -0.00476074\n",
      " -0.0062565  -0.00474028]\n",
      "Similarity between 'machine' and 'learning': 0.11255005\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec  # Import Word2Vec\n",
    "from nltk.tokenize import word_tokenize  # Tokenizer for splitting sentences\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    \"Natural language processing is fun\",\n",
    "    \"Machine learning is a part of artificial intelligence\",\n",
    "    \"Word embeddings capture semantic meaning\"\n",
    "]\n",
    "\n",
    "# Tokenize the corpus (split each sentence into words)\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train the Word2Vec model using Skip-gram (sg=1) with vector size of 50\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=50, window=2, min_count=1, sg=1)\n",
    "\n",
    "# Get the embedding vector for the word 'machine'\n",
    "print(\"Embedding for 'machine':\\n\", model.wv['machine'])\n",
    "\n",
    "# Check similarity between two words\n",
    "print(\"Similarity between 'machine' and 'learning':\", model.wv.similarity('machine', 'learning'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ GloVe Embeddings\n",
    "\n",
    "Idea: Uses global word co-occurrence statistics across the entire corpus instead of local context windows.\n",
    "\n",
    "Example: Using Pretrained GloVe Embeddings (Common Crawl or Wikipedia):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embedding for 'machine':\n",
      " [-0.34165   -0.81267    1.4513     0.05914   -0.080801   0.39567\n",
      "  0.10064   -0.5468    -0.18887    0.11364   -0.040956  -0.5637\n",
      " -0.32191    0.15968   -0.59756   -0.14571   -0.77074    1.2955\n",
      " -0.72002   -0.90818    0.76644    0.05346   -0.0031632 -0.15341\n",
      "  0.22065   -1.191     -1.0775    -0.29768    1.327     -0.51359\n",
      "  2.6229    -0.67411   -0.82558    0.14283   -0.014214   0.90775\n",
      "  0.66828    0.48431    0.1543     0.26044    1.0191     0.015872\n",
      " -0.75325    0.58992    0.4546    -0.19678    0.42138   -0.43168\n",
      "  0.11985    0.14094  ]\n"
     ]
    }
   ],
   "source": [
    "# Download pretrained GloVe from: https://nlp.stanford.edu/projects/glove/\n",
    "# Example: 'glove.6B.50d.txt' contains 50-dimensional vectors\n",
    "# the word 'to' in txt file has the value as below : \n",
    "# to = [0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 .... -0.064699 -0.26044] with the length of 50\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load the GloVe embeddings (assuming the file 'glove.6B.50d.txt' is downloaded)\n",
    "glove_embeddings = {}\n",
    "with open('data/glove.6B.50d.txt', 'r', encoding='utf-8') as f: # text file saved in 'data' folder\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "# Example: Get embedding for 'machine'\n",
    "print(\"GloVe embedding for 'machine':\\n\", glove_embeddings.get('machine'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ FastText Embeddings\n",
    "\n",
    "Idea: Considers subword information (character n-grams), helping handle out-of-vocabulary (OOV) words better than Word2Vec and GloVe.\n",
    "\n",
    "FastText can generate embeddings for unseen words based on their subword components.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText embedding for 'machine':\n",
      " [-1.8079143e-03  2.1979981e-03 -1.7691230e-03  5.6677201e-04\n",
      "  1.2654356e-03  2.1495651e-03 -1.5551907e-03  4.3800026e-03\n",
      " -2.0496619e-03  5.3299527e-04 -2.5257603e-03  1.6439921e-03\n",
      "  3.4959912e-03 -9.7273325e-05  2.1534071e-03  2.0341277e-03\n",
      " -5.6405651e-04  1.8103337e-03  4.1687866e-03  7.4633321e-04\n",
      " -3.0441333e-03 -3.0280279e-03  3.9849104e-03 -7.3370530e-04\n",
      " -1.7331528e-03  1.6396311e-03 -6.8702095e-04 -2.2539324e-03\n",
      "  5.6145241e-04  1.4721482e-03 -2.8888162e-03 -2.2243629e-03\n",
      "  2.1639713e-03 -1.2766268e-03  6.0394765e-03  4.9851830e-03\n",
      "  3.3022531e-03  1.5956949e-03 -4.3668048e-03  1.5206628e-03\n",
      " -2.3396676e-03  7.1912521e-04  2.4290388e-03 -5.5817286e-03\n",
      "  2.9966333e-03 -6.4665275e-03 -5.4450257e-04 -2.2184665e-03\n",
      "  1.3568229e-03 -5.2718865e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText  # Import FastText\n",
    "\n",
    "# Use the same tokenized corpus\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train FastText model\n",
    "fasttext_model = FastText(sentences=tokenized_sentences, vector_size=50, window=2, min_count=1)\n",
    "\n",
    "# Get vector for a word\n",
    "print(\"FastText embedding for 'machine':\\n\", fasttext_model.wv['machine'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Why Use Word Embeddings?\n",
    "\n",
    "| Method          | Sparse/Dense    | Captures Word Meaning? | Handles OOV Words?      |\n",
    "|-----------------|-----------------|-----------------------|------------------------|\n",
    "| BoW / TF-IDF    | Sparse           | ‚ùå No                  | ‚ùå No                   |\n",
    "| Word2Vec        | Dense            | ‚úÖ Yes                 | ‚ùå No                   |\n",
    "| GloVe           | Dense            | ‚úÖ Yes                 | ‚ùå No                   |\n",
    "| FastText        | Dense            | ‚úÖ Yes                 | ‚úÖ Yes (via subwords)   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Answers to Practice Questions (Word Embeddings)\n",
    "\n",
    "### 1Ô∏è‚É£ Why are word embeddings better than BoW or TF-IDF for capturing meaning?\n",
    "Word embeddings (like Word2Vec, GloVe, FastText) map words into dense vectors where **semantically similar words are closer together in the vector space**. Unlike BoW or TF-IDF, which only count word occurrences and ignore word order or meaning, embeddings capture relationships between words (e.g., \"king\" is close to \"queen\", \"Paris\" is close to \"France\").\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ What is the main difference between Skip-gram and CBOW in Word2Vec?\n",
    "- **CBOW (Continuous Bag of Words):** Predicts the target word based on its surrounding context words.\n",
    "- **Skip-gram:** Predicts the surrounding context words given the target word.\n",
    "- Typically, **Skip-gram works better for small datasets** and rare words, while **CBOW is faster on large datasets**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ How does FastText handle words that it has not seen during training?\n",
    "FastText breaks words into **subword units (character n-grams)**. This allows it to create word vectors by combining the vectors of these subwords. Even if a word was not in the training data (out-of-vocabulary, OOV), FastText can generate a vector based on its subword pieces, making it more robust to rare or unseen words.\n",
    "\n",
    "Example:  \n",
    "The word **\"running\"** may be broken into subwords like `\"run\"`, `\"unn\"`, `\"nni\"`, `\"nin\"`, `\"ing\"`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
