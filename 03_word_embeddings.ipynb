{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåê Chapter 3: Word Embeddings\n",
    "\n",
    "## üìå Overview  \n",
    "Bag of Words (BoW) and TF-IDF represent words as sparse vectors where each dimension corresponds to a unique word. However, they fail to capture **semantic relationships** between words (e.g., \"king\" and \"queen\").\n",
    "\n",
    "**Word Embeddings** solve this problem by mapping words into dense, low-dimensional vectors where similar words have similar representations. These embeddings are learned from large text corpora.\n",
    "\n",
    "Popular embedding methods:\n",
    "- Word2Vec (Skip-gram, CBOW)\n",
    "- GloVe (Global Vectors)\n",
    "- FastText\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Introduction to Word2Vec  \n",
    "**Idea:** Words that appear in similar contexts have similar embeddings (distributional hypothesis).  \n",
    "- **CBOW (Continuous Bag of Words):** Predicts a word from surrounding context.  \n",
    "- **Skip-gram:** Predicts surrounding context from a given word.\n",
    "\n",
    "**Example using `gensim`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.3 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Downloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /Users/moka/Library/Python/3.9/lib/python/site-packages (from gensim) (1.13.1)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
      "  Downloading wrapt-1.17.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Downloading gensim-4.3.3-cp39-cp39-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp39-cp39-macosx_11_0_arm64.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading wrapt-1.17.2-cp39-cp39-macosx_11_0_arm64.whl (38 kB)\n",
      "Installing collected packages: wrapt, numpy, smart-open, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "Successfully installed gensim-4.3.3 numpy-1.26.4 smart-open-7.1.0 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moka/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'machine':\n",
      " [-0.01648536  0.01859871 -0.00039532 -0.00393455  0.00920726 -0.00819063\n",
      "  0.00548623  0.01387993  0.01213085 -0.01502159  0.0187647   0.00934362\n",
      "  0.00793224 -0.01248701  0.01691996 -0.00430033  0.01765038 -0.01072401\n",
      " -0.01625884  0.01364912  0.00334239 -0.00439702  0.0190272   0.01898771\n",
      " -0.01954809  0.00501046  0.01231338  0.00774491  0.00404557  0.000861\n",
      "  0.00134726 -0.00764127 -0.0142805  -0.00417774  0.0078478   0.01763737\n",
      "  0.0185183  -0.01195187 -0.01880534  0.01952875  0.00685957  0.01033223\n",
      "  0.01256469 -0.00560853  0.01464541  0.00566054  0.00574201 -0.00476074\n",
      " -0.0062565  -0.00474028]\n",
      "Similarity between 'machine' and 'learning': 0.11255005\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec  # Import Word2Vec\n",
    "from nltk.tokenize import word_tokenize  # Tokenizer for splitting sentences\n",
    "\n",
    "# Sample corpus\n",
    "sentences = [\n",
    "    \"Natural language processing is fun\",\n",
    "    \"Machine learning is a part of artificial intelligence\",\n",
    "    \"Word embeddings capture semantic meaning\"\n",
    "]\n",
    "\n",
    "# Tokenize the corpus (split each sentence into words)\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train the Word2Vec model using Skip-gram (sg=1) with vector size of 50\n",
    "model = Word2Vec(sentences=tokenized_sentences, vector_size=50, window=2, min_count=1, sg=1)\n",
    "\n",
    "# Get the embedding vector for the word 'machine'\n",
    "print(\"Embedding for 'machine':\\n\", model.wv['machine'])\n",
    "\n",
    "# Check similarity between two words\n",
    "print(\"Similarity between 'machine' and 'learning':\", model.wv.similarity('machine', 'learning'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ GloVe Embeddings\n",
    "\n",
    "Idea: Uses global word co-occurrence statistics across the entire corpus instead of local context windows.\n",
    "\n",
    "Example: Using Pretrained GloVe Embeddings (Common Crawl or Wikipedia):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embedding for 'machine':\n",
      " [-0.34165   -0.81267    1.4513     0.05914   -0.080801   0.39567\n",
      "  0.10064   -0.5468    -0.18887    0.11364   -0.040956  -0.5637\n",
      " -0.32191    0.15968   -0.59756   -0.14571   -0.77074    1.2955\n",
      " -0.72002   -0.90818    0.76644    0.05346   -0.0031632 -0.15341\n",
      "  0.22065   -1.191     -1.0775    -0.29768    1.327     -0.51359\n",
      "  2.6229    -0.67411   -0.82558    0.14283   -0.014214   0.90775\n",
      "  0.66828    0.48431    0.1543     0.26044    1.0191     0.015872\n",
      " -0.75325    0.58992    0.4546    -0.19678    0.42138   -0.43168\n",
      "  0.11985    0.14094  ]\n"
     ]
    }
   ],
   "source": [
    "# Download pretrained GloVe from: https://nlp.stanford.edu/projects/glove/\n",
    "# Example: 'glove.6B.50d.txt' contains 50-dimensional vectors\n",
    "# the word 'to' in txt file has the value as below : \n",
    "# to = [0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 .... -0.064699 -0.26044] with the length of 50\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load the GloVe embeddings (assuming the file 'glove.6B.50d.txt' is downloaded)\n",
    "glove_embeddings = {}\n",
    "with open('data/glove.6B.50d.txt', 'r', encoding='utf-8') as f: # text file saved in 'data' folder\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_embeddings[word] = vector\n",
    "\n",
    "# Example: Get embedding for 'machine'\n",
    "print(\"GloVe embedding for 'machine':\\n\", glove_embeddings.get('machine'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ FastText Embeddings\n",
    "\n",
    "Idea: Considers subword information (character n-grams), helping handle out-of-vocabulary (OOV) words better than Word2Vec and GloVe.\n",
    "\n",
    "FastText can generate embeddings for unseen words based on their subword components.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText embedding for 'machine':\n",
      " [-1.8079143e-03  2.1979981e-03 -1.7691230e-03  5.6677201e-04\n",
      "  1.2654356e-03  2.1495651e-03 -1.5551907e-03  4.3800026e-03\n",
      " -2.0496619e-03  5.3299527e-04 -2.5257603e-03  1.6439921e-03\n",
      "  3.4959912e-03 -9.7273325e-05  2.1534071e-03  2.0341277e-03\n",
      " -5.6405651e-04  1.8103337e-03  4.1687866e-03  7.4633321e-04\n",
      " -3.0441333e-03 -3.0280279e-03  3.9849104e-03 -7.3370530e-04\n",
      " -1.7331528e-03  1.6396311e-03 -6.8702095e-04 -2.2539324e-03\n",
      "  5.6145241e-04  1.4721482e-03 -2.8888162e-03 -2.2243629e-03\n",
      "  2.1639713e-03 -1.2766268e-03  6.0394765e-03  4.9851830e-03\n",
      "  3.3022531e-03  1.5956949e-03 -4.3668048e-03  1.5206628e-03\n",
      " -2.3396676e-03  7.1912521e-04  2.4290388e-03 -5.5817286e-03\n",
      "  2.9966333e-03 -6.4665275e-03 -5.4450257e-04 -2.2184665e-03\n",
      "  1.3568229e-03 -5.2718865e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText  # Import FastText\n",
    "\n",
    "# Use the same tokenized corpus\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train FastText model\n",
    "fasttext_model = FastText(sentences=tokenized_sentences, vector_size=50, window=2, min_count=1)\n",
    "\n",
    "# Get vector for a word\n",
    "print(\"FastText embedding for 'machine':\\n\", fasttext_model.wv['machine'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Why Use Word Embeddings?\n",
    "\n",
    "| Method          | Sparse/Dense    | Captures Word Meaning? | Handles OOV Words?      |\n",
    "|-----------------|-----------------|-----------------------|------------------------|\n",
    "| BoW / TF-IDF    | Sparse           | ‚ùå No                  | ‚ùå No                   |\n",
    "| Word2Vec        | Dense            | ‚úÖ Yes                 | ‚ùå No                   |\n",
    "| GloVe           | Dense            | ‚úÖ Yes                 | ‚ùå No                   |\n",
    "| FastText        | Dense            | ‚úÖ Yes                 | ‚úÖ Yes (via subwords)   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Answers to Practice Questions (Word Embeddings)\n",
    "\n",
    "### 1Ô∏è‚É£ Why are word embeddings better than BoW or TF-IDF for capturing meaning?\n",
    "Word embeddings (like Word2Vec, GloVe, FastText) map words into dense vectors where **semantically similar words are closer together in the vector space**. Unlike BoW or TF-IDF, which only count word occurrences and ignore word order or meaning, embeddings capture relationships between words (e.g., \"king\" is close to \"queen\", \"Paris\" is close to \"France\").\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ What is the main difference between Skip-gram and CBOW in Word2Vec?\n",
    "- **CBOW (Continuous Bag of Words):** Predicts the target word based on its surrounding context words.\n",
    "- **Skip-gram:** Predicts the surrounding context words given the target word.\n",
    "- Typically, **Skip-gram works better for small datasets** and rare words, while **CBOW is faster on large datasets**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ How does FastText handle words that it has not seen during training?\n",
    "FastText breaks words into **subword units (character n-grams)**. This allows it to create word vectors by combining the vectors of these subwords. Even if a word was not in the training data (out-of-vocabulary, OOV), FastText can generate a vector based on its subword pieces, making it more robust to rare or unseen words.\n",
    "\n",
    "Example:  \n",
    "The word **\"running\"** may be broken into subwords like `\"run\"`, `\"unn\"`, `\"nni\"`, `\"nin\"`, `\"ing\"`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
