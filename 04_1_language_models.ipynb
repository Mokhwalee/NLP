{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìñ Chapter 4.1: Language Models\n",
    "\n",
    "## üìå Overview  \n",
    "A **Language Model (LM)** estimates the probability of a sequence of words.  \n",
    "It answers the question:  \n",
    "> \"Given the previous words, what is the likelihood of the next word?\"\n",
    "\n",
    "Language models are the backbone of many NLP tasks like:\n",
    "- Text generation\n",
    "- Speech recognition\n",
    "- Machine translation\n",
    "- Autocomplete and chatbots\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Probability of Word Sequences  \n",
    "The **chain rule of probability** defines the probability of a sequence:\n",
    "\\[\n",
    "P(w_1, w_2, w_3, ..., w_n) = P(w_1) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_1, w_2) \\cdots P(w_n | w_1, ..., w_{n-1})\n",
    "\\]\n",
    "\n",
    "Since modeling all previous words is computationally expensive, **n-gram models** simplify this by assuming that:\n",
    "\\[\n",
    "P(w_n | w_1, ..., w_{n-1}) \\approx P(w_n | w_{n-(n-1)}, ..., w_{n-1})\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ N-gram Language Models  \n",
    "**N-gram:** A contiguous sequence of `n` words.\n",
    "\n",
    "| N-gram Type     | Example Phrase              |\n",
    "|-----------------|----------------------------|\n",
    "| Unigram (n=1)   | \"The\", \"dog\", \"runs\"        |\n",
    "| Bigram (n=2)    | \"The dog\", \"dog runs\"       |\n",
    "| Trigram (n=3)   | \"The dog runs\", \"dog runs fast\" |\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Example: Building a Bigram Model using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Frequencies:\n",
      " [(('natural', 'language'), 1), (('language', 'processing'), 1), (('processing', 'makes'), 1), (('makes', 'machines'), 1), (('machines', 'understand'), 1), (('understand', 'human'), 1), (('human', 'language'), 1), (('language', '.'), 1)]\n",
      "Words that follow 'language': [('processing', 1), ('.', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/moka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural language processing makes machines understand human language.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "# Generate bigrams from the token list\n",
    "bigrams_list = list(bigrams(tokens))\n",
    "\n",
    "# Frequency distribution of bigrams\n",
    "fdist = FreqDist(bigrams_list)\n",
    "print(\"Bigram Frequencies:\\n\", fdist.most_common())\n",
    "\n",
    "# Conditional Frequency Distribution: What words often follow 'language'?\n",
    "cfd = ConditionalFreqDist(bigrams_list)\n",
    "print(\"Words that follow 'language':\", cfd['language'].most_common())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ Limitations of N-gram Models\n",
    "\n",
    "Only captures short-range dependencies.\n",
    "Suffers from the curse of dimensionality (sparse data problem).\n",
    "Requires smoothing techniques (e.g., Laplace smoothing) to handle unseen n-grams.\n",
    "\n",
    "# 3Ô∏è‚É£ Moving Beyond N-grams: Neural Language Models\n",
    "\n",
    "To overcome these limitations, neural network-based models were introduced:\n",
    "\n",
    "Feedforward Neural Network Language Model (Bengio et al., 2003)\n",
    "Recurrent Neural Networks (RNNs)\n",
    "Long Short-Term Memory (LSTM)\n",
    "Transformer-based models (e.g., GPT, BERT)\n",
    "These models can learn:\n",
    "\n",
    "Long-range dependencies\n",
    "Better generalization for unseen word combinations\n",
    "(You‚Äôll explore these in detail in the next sections!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practice Questions: (Language Models)\n",
    "\n",
    "### 1Ô∏è‚É£ What is the main assumption made by n-gram models to simplify the computation of word sequence probabilities?\n",
    "N-gram models assume the **Markov property**, which means the probability of a word depends only on the previous `n-1` words (not the entire history).  \n",
    "For example, in a bigram model:\n",
    "\\[\n",
    "P(w_n | w_1, w_2, ..., w_{n-1}) \\approx P(w_n | w_{n-1})\n",
    "\\]\n",
    "This greatly reduces complexity but ignores longer context.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Why do n-gram models struggle with long sentences?\n",
    "N-gram models struggle with long sentences because they:\n",
    "- Only consider **short-range dependencies** (limited by the value of `n`).\n",
    "- Cannot remember earlier parts of a sentence beyond `n-1` words.\n",
    "- Suffer from **data sparsity** as the number of possible n-grams grows exponentially with `n`, making it hard to cover all combinations in the training data.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ How do neural language models improve over traditional n-gram models?\n",
    "Neural language models (like feedforward networks, RNNs, LSTMs, Transformers) improve over n-gram models by:\n",
    "- **Learning distributed word representations (embeddings)** that capture semantic relationships.\n",
    "- **Handling long-range dependencies** through architectures like RNNs and Transformers.\n",
    "- **Generalizing better** to unseen word sequences using learned parameters, rather than relying on explicit counting.\n",
    "- **Reducing the curse of dimensionality** by mapping words to dense vectors instead of one-hot encoding.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
