{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìñ Chapter 4.1: Language Models\n",
    "\n",
    "## üìå Overview  \n",
    "A **Language Model (LM)** estimates the probability of a sequence of words.  \n",
    "It answers the question:  \n",
    "> \"Given the previous words, what is the likelihood of the next word?\"\n",
    "\n",
    "Language models are the backbone of many NLP tasks like:\n",
    "- Text generation\n",
    "- Speech recognition\n",
    "- Machine translation\n",
    "- Autocomplete and chatbots\n",
    "\n",
    "---\n",
    "\n",
    "## üî¢ Probability of Word Sequences  \n",
    "The **chain rule of probability** defines the probability of a sequence:\n",
    "$$\n",
    "P(w_1, w_2, w_3, ..., w_n) = P(w_1) \\cdot P(w_2 | w_1) \\cdot P(w_3 | w_1, w_2) \\cdots P(w_n | w_1, ..., w_{n-1})\n",
    "$$\n",
    "\n",
    "Since modeling all previous words is computationally expensive, **n-gram models** simplify this by assuming that:\n",
    "$$\n",
    "P(w_n | w_1, ..., w_{n-1}) \\approx P(w_n | w_{n-(n-1)}, ..., w_{n-1})\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ N-gram Language Models  \n",
    "**N-gram:** A contiguous sequence of `n` words.\n",
    "\n",
    "| N-gram Type     | Example Phrase              |\n",
    "|-----------------|----------------------------|\n",
    "| Unigram (n=1)   | \"The\", \"dog\", \"runs\"        |\n",
    "| Bigram (n=2)    | \"The dog\", \"dog runs\"       |\n",
    "| Trigram (n=3)   | \"The dog runs\", \"dog runs fast\" |\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Example: Building a Bigram Model using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Frequencies:\n",
      " [(('natural', 'language'), 1), (('language', 'processing'), 1), (('processing', 'makes'), 1), (('makes', 'machines'), 1), (('machines', 'understand'), 1), (('understand', 'human'), 1), (('human', 'language'), 1), (('language', '.'), 1)]\n",
      "Words that follow 'language': [('processing', 1), ('.', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/moka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import bigrams\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"Natural language processing makes machines understand human language.\"\n",
    "\n",
    "# Tokenize the text into words\n",
    "tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "# Generate bigrams from the token list\n",
    "bigrams_list = list(bigrams(tokens))\n",
    "\n",
    "# Frequency distribution of bigrams\n",
    "fdist = FreqDist(bigrams_list)\n",
    "print(\"Bigram Frequencies:\\n\", fdist.most_common())\n",
    "\n",
    "# Conditional Frequency Distribution: What words often follow 'language'?\n",
    "cfd = ConditionalFreqDist(bigrams_list)\n",
    "print(\"Words that follow 'language':\", cfd['language'].most_common())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Limitations of N-gram Models\n",
    "\n",
    "Only captures short-range dependencies.\n",
    "Suffers from the curse of dimensionality (sparse data problem).\n",
    "Requires smoothing techniques (e.g., Laplace smoothing) to handle unseen n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practice Questions: (Language Models)\n",
    "\n",
    "### 1Ô∏è. What is the main assumption made by n-gram models to simplify the computation of word sequence probabilities?\n",
    "N-gram models assume the **Markov property**, which means the probability of a word depends only on the previous `n-1` words (not the entire history).  \n",
    "For example, in a bigram model:\n",
    "$$\n",
    "P(w_n | w_1, w_2, ..., w_{n-1}) \\approx P(w_n | w_{n-1})\n",
    "$$\n",
    "This greatly reduces complexity but ignores longer context.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è. Why do n-gram models struggle with long sentences?\n",
    "N-gram models struggle with long sentences because they:\n",
    "- Only consider **short-range dependencies** (limited by the value of `n`).\n",
    "- Cannot remember earlier parts of a sentence beyond `n-1` words.\n",
    "- Suffer from **data sparsity** as the number of possible n-grams grows exponentially with `n`, making it hard to cover all combinations in the training data.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è. How do neural language models improve over traditional n-gram models?\n",
    "Neural language models (like feedforward networks, RNNs, LSTMs, Transformers) improve over n-gram models by:\n",
    "- **Learning distributed word representations (embeddings)** that capture semantic relationships.\n",
    "- **Handling long-range dependencies** through architectures like RNNs and Transformers.\n",
    "- **Generalizing better** to unseen word sequences using learned parameters, rather than relying on explicit counting.\n",
    "- **Reducing the curse of dimensionality** by mapping words to dense vectors instead of one-hot encoding.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ Moving Beyond N-grams: Neural Language Models\n",
    "\n",
    "To overcome these limitations, neural network-based models were introduced:\n",
    "\n",
    "- Feedforward Neural Network Language Model (Bengio et al., 2003)\n",
    "\n",
    "- Recurrent Neural Networks (RNNs)\n",
    "\n",
    "- Long Short-Term Memory (LSTM)\n",
    "\n",
    "- Transformer-based models (e.g., GPT, BERT)\n",
    "\n",
    "- These models can learn:\n",
    "\n",
    "- Long-range dependencies\n",
    "Better generalization for unseen word combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Neural Language Models\n",
    "\n",
    "## üìå Overview  \n",
    "Traditional **n-gram language models** rely on counting word sequences but struggle with:\n",
    "- Long-range dependencies\n",
    "- Data sparsity\n",
    "- High-dimensional, sparse word vectors (one-hot encoding)\n",
    "\n",
    "To address these limitations, **neural language models** use neural networks and word embeddings to learn:\n",
    "- Dense vector representations of words\n",
    "- Probability distributions over the next word, conditioned on previous words\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Feedforward Neural Network Language Model (NNLM)  \n",
    "**Introduced by:** Bengio et al., 2003  \n",
    "**Idea:**  \n",
    "- Uses a fixed-size context window (like n-grams).  \n",
    "- Concatenates the embeddings of previous words as input to a feedforward neural network.  \n",
    "- Outputs the probability of the next word.\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Structure of NNLM:\n",
    "1. Input: Embeddings of the last `n-1` words  \n",
    "2. Hidden layer: Nonlinear transformation (e.g., ReLU, tanh)  \n",
    "3. Output layer: Softmax over the vocabulary for next-word prediction\n",
    "\n",
    "> ‚ö†Ô∏è Limitation: Fixed context window size ‚Üí still struggles with long dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardNNLM(\n",
      "  (embeddings): Embedding(1000, 50)\n",
      "  (linear1): Linear(in_features=100, out_features=128, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (linear2): Linear(in_features=128, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Feedforward NNLM: Predict next word based on fixed context (e.g., bigram)\n",
    "class FeedforwardNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
    "        super(FeedforwardNNLM, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Get embeddings and flatten (concatenate embeddings of context words)\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# Example\n",
    "model = FeedforwardNNLM(vocab_size=1000, embedding_dim=50, context_size=2, hidden_dim=128)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Recurrent Neural Networks (RNNs)  \n",
    "**Motivation:** Process **sequences of arbitrary length** by maintaining a **hidden state** that summarizes past information.\n",
    "\n",
    "### üåÄ How RNN works:\n",
    "At each time step `t`:\n",
    "$$\n",
    "h_t = f(W_{hh} h_{t-1} + W_{xh} x_t + b)\n",
    "$$\n",
    "- `h_t`: Hidden state at time `t` (memory of the sequence)\n",
    "- `x_t`: Input embedding at time `t`\n",
    "- `f`: Nonlinear activation (like tanh or ReLU)\n",
    "\n",
    "The output at each step predicts the next word:\n",
    "$$\n",
    "\\hat{y}_t = \\text{softmax}(W_{hy} h_t + b_y)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Why RNNs Are Better Than N-gram Models:\n",
    "| Model             | Handles Variable Length? | Captures Long Dependencies? | Requires Word Counting? |\n",
    "|-------------------|-------------------------|----------------------------|------------------------|\n",
    "| N-gram            | ‚ùå No                    | ‚ùå Short context only       | ‚úÖ Yes                 |\n",
    "| Feedforward NNLM  | ‚ùå No (fixed window)     | ‚ùå Limited                 | ‚ùå No (uses embeddings) |\n",
    "| RNN               | ‚úÖ Yes                   | ‚úÖ Yes (but can suffer from vanishing gradients) | ‚ùå No                 |\n",
    "\n",
    "---\n",
    "\n",
    "## üö© Limitation of Vanilla RNNs:\n",
    "- **Vanishing gradient problem**: Hard to learn long-range dependencies.\n",
    "- Solution: Introduce **LSTM (Long Short-Term Memory)** and **GRU (Gated Recurrent Unit)** ‚Äî covered in the next section!\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Simple Example: RNN with PyTorch (Optional Code Example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN(\n",
      "  (embedding): Embedding(1000, 50)\n",
      "  (rnn): RNN(50, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple RNN model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "# Example parameters\n",
    "vocab_size = 1000\n",
    "embedding_dim = 50\n",
    "hidden_dim = 128\n",
    "\n",
    "model = SimpleRNN(vocab_size, embedding_dim, hidden_dim)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ Long Short-Term Memory (LSTM)\n",
    "**Proposed by:** Hochreiter & Schmidhuber, 1997  \n",
    "**Motivation:** Solve the **vanishing gradient problem** of standard RNNs.\n",
    "\n",
    "**Key Components (Gates):**\n",
    "- **Forget gate:** Decides what information to discard from the cell state.\n",
    "- **Input gate:** Decides what new information to store.\n",
    "- **Output gate:** Controls what part of the cell state goes to the output.\n",
    "\n",
    "$$\n",
    "c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\n",
    "$$\n",
    "Where:\n",
    "- `c_t`: Cell state\n",
    "- `f_t`: Forget gate\n",
    "- `i_t`: Input gate\n",
    "- `\\tilde{c}_t`: Candidate values\n",
    "- `\\odot`: Element-wise multiplication\n",
    "\n",
    "‚úÖ **Advantage:** Can remember information over long sequences.  \n",
    "üü° Often used in language modeling, machine translation, and time-series forecasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleLSTMModel(\n",
      "  (embedding): Embedding(1000, 50)\n",
      "  (lstm): LSTM(50, 128, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(SimpleLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "lstm_model = SimpleLSTMModel(vocab_size=1000, embedding_dim=50, hidden_dim=128)\n",
    "print(lstm_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4Ô∏è‚É£ Transformer-based Models\n",
    "**Proposed by:** Vaswani et al., 2017 (‚ÄúAttention Is All You Need‚Äù)  \n",
    "\n",
    "**Key Concept:**  \n",
    "- Uses **self-attention mechanisms** instead of recurrence to capture dependencies across the entire sequence.\n",
    "- Processes the sequence **in parallel** (unlike RNNs which are sequential).\n",
    "\n",
    "**Structure:**\n",
    "- **Positional encodings** added to input embeddings (since no recurrence).\n",
    "- Stacked **multi-head self-attention layers** and feedforward layers.\n",
    "- Variants:\n",
    "  - **GPT (Generative Pre-trained Transformer):** Decoder-only, causal attention (for text generation).\n",
    "  - **BERT (Bidirectional Encoder Representations from Transformers):** Encoder-only, masked language modeling (for text understanding tasks like classification, Q&A).\n",
    "\n",
    "‚úÖ **Advantage:** Handles **long-range dependencies** efficiently and allows **parallel training**.  \n",
    "‚ö° **Transformers are now the state-of-the-art** in many NLP tasks.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleTransformerModel(\n",
      "  (embedding): Embedding(1000, 50)\n",
      "  (transformer_decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=50, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=50, bias=True)\n",
      "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=50, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_heads, hidden_dim, num_layers):\n",
    "        super(SimpleTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, 100, embedding_dim))  # Simplified positional encoding\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, memory):\n",
    "        embedded = self.embedding(x) + self.pos_encoder[:, :x.size(1), :]  # Add position encoding\n",
    "        output = self.transformer_decoder(embedded, memory)\n",
    "        logits = self.fc(output)\n",
    "        return logits\n",
    "\n",
    "transformer_model = SimpleTransformerModel(vocab_size=1000, embedding_dim=50, num_heads=2, hidden_dim=128, num_layers=2)\n",
    "print(transformer_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Summary of Neural Language Models\n",
    "\n",
    "| Model                        | Handles Long Dependencies? | Fixed Context? | Parallelizable? | Common Use Cases                     |\n",
    "|------------------------------|---------------------------|----------------|----------------|---------------------------------------|\n",
    "| Feedforward NNLM             | ‚ùå No                      | ‚úÖ Yes         | ‚úÖ Yes         | Basic word prediction               |\n",
    "| RNN                          | ‚úÖ Yes (but limited)      | ‚ùå No          | ‚ùå No          | Sequential data, speech, text gen    |\n",
    "| LSTM                         | ‚úÖ Yes (solves RNN issues)| ‚ùå No          | ‚ùå No          | Long text sequences, translation     |\n",
    "| Transformer (GPT, BERT, etc.)| ‚úÖ Yes (long dependencies)| ‚ùå No          | ‚úÖ Yes         | Text gen, classification, translation|\n",
    "\n",
    "---\n",
    "## ‚úÖ Recap of Input and Output Shapes\n",
    "\n",
    "| Model              | Input Shape                     | Output Shape                          |\n",
    "|--------------------|----------------------------------|----------------------------------------|\n",
    "| Feedforward NNLM   | `[context_size]`                | `[1, vocab_size]`                     |\n",
    "| RNN                | `[batch_size, seq_length]`      | `[batch_size, seq_length, vocab_size]`|\n",
    "| LSTM               | `[batch_size, seq_length]`      | `[batch_size, seq_length, vocab_size]`|\n",
    "| Transformer        | `decoder_input + encoder memory`| `[batch_size, seq_length, vocab_size]`|\n",
    "\n",
    "---\n",
    "## üéØ Why These Models Matter:\n",
    "- Move beyond simple word counts.\n",
    "- Learn **contextual relationships** between words.\n",
    "- Enable **state-of-the-art NLP applications** like ChatGPT, BERT-based Q&A systems, machine translation, text summarization, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Practice Questions:(Neural Language Models)\n",
    "\n",
    "### 1Ô∏è‚É£ What is the main advantage of neural language models over n-gram models?\n",
    "Neural language models (like feedforward NNLMs and RNNs) avoid explicit counting of word sequences.  \n",
    "Instead, they:\n",
    "- Use **word embeddings** (dense vectors) to capture semantic meaning.\n",
    "- Generalize better to **unseen word combinations**.\n",
    "- Handle **large vocabularies** more efficiently.\n",
    "- Do not suffer from the **curse of dimensionality** like n-grams.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Why do RNNs use hidden states, and what problem does this solve?\n",
    "RNNs maintain a **hidden state** that carries forward information from previous time steps.  \n",
    "This allows RNNs to:\n",
    "- **Summarize the history** of the sequence up to the current word.\n",
    "- Handle **sequences of arbitrary length**.\n",
    "- Learn **temporal patterns** or dependencies across words.\n",
    "\n",
    "The hidden state helps solve the problem that n-gram and feedforward models face ‚Äî they only see a **fixed window** of context.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ What is the major limitation of vanilla RNNs, and how can it be addressed?\n",
    "The biggest limitation of vanilla RNNs is the **vanishing (or exploding) gradient problem**, which makes it hard to learn **long-range dependencies** during backpropagation through time (BPTT).\n",
    "\n",
    "**Solution:**  \n",
    "Use advanced RNN variants:\n",
    "- **LSTM (Long Short-Term Memory)**: Adds gates (input, forget, output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
